import streamlit as st
import torch
import math
from utils.common import np_to_df

def render_pytorch_implementation(analyze, src_tokens, tgt_tokens, src_E, tgt_E, Wq, Wk, Wv):
    """PyTorch êµ¬í˜„ íƒ­ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    
    st.subheader("PyTorchë¡œ ìŠ¤ì¼€ì¼ë“œë‹·í”„ë¡œë•íŠ¸ & ë©€í‹°í—¤ë“œ")
    
    if analyze:
        # PyTorch í…ì„œë¡œ ë³€í™˜
        src_E_torch = torch.tensor(src_E, dtype=torch.float32)
        tgt_E_torch = torch.tensor(tgt_E, dtype=torch.float32)
        Wq_torch = torch.tensor(Wq, dtype=torch.float32)
        Wk_torch = torch.tensor(Wk, dtype=torch.float32)
        Wv_torch = torch.tensor(Wv, dtype=torch.float32)
        
        # PyTorchë¡œ ì–´í…ì…˜ ê³„ì‚°
        Q_torch = torch.matmul(src_E_torch, Wq_torch)
        K_torch = torch.matmul(src_E_torch, Wk_torch)
        V_torch = torch.matmul(src_E_torch, Wv_torch)
        
        # Scaled Dot-Product Attention
        d_k = Q_torch.size(-1)
        scores_torch = torch.matmul(Q_torch, K_torch.transpose(-2, -1)) / math.sqrt(d_k)
        weights_torch = torch.softmax(scores_torch, dim=-1)
        context_torch = torch.matmul(weights_torch, V_torch)
        
        st.write("PyTorch ì–´í…ì…˜ ê²°ê³¼ shape:", context_torch.shape)
        st.dataframe(np_to_df(context_torch.detach().numpy(), row_idx=src_tokens))
        
        # ì¶”ê°€ì ì¸ PyTorch ê¸°ëŠ¥ë“¤
        st.markdown("### ğŸ”§ PyTorch ê³ ê¸‰ ê¸°ëŠ¥")
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        if st.checkbox("ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° í™œì„±í™”"):
            src_E_torch.requires_grad_(True)
            Wq_torch.requires_grad_(True)
            
            # ì–´í…ì…˜ ê³„ì‚° (ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì )
            Q_torch_grad = torch.matmul(src_E_torch, Wq_torch)
            K_torch_grad = torch.matmul(src_E_torch, Wk_torch)
            V_torch_grad = torch.matmul(src_E_torch, Wv_torch)
            
            scores_torch_grad = torch.matmul(Q_torch_grad, K_torch_grad.transpose(-2, -1)) / math.sqrt(d_k)
            weights_torch_grad = torch.softmax(scores_torch_grad, dim=-1)
            context_torch_grad = torch.matmul(weights_torch_grad, V_torch_grad)
            
            # ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆì‹œ)
            target = torch.randn_like(context_torch_grad)
            loss = torch.nn.functional.mse_loss(context_torch_grad, target)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
            loss.backward()
            
            st.write("**ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´:**")
            st.write(f"- src_E ê·¸ë˜ë””ì–¸íŠ¸ shape: {src_E_torch.grad.shape}")
            st.write(f"- Wq ê·¸ë˜ë””ì–¸íŠ¸ shape: {Wq_torch.grad.shape}")
            st.write(f"- ì†ì‹¤ê°’: {loss.item():.6f}")
        
        # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ êµ¬í˜„
        st.markdown("### ğŸ§© PyTorch ë©€í‹°í—¤ë“œ ì–´í…ì…˜")
        
        num_heads = st.slider("í—¤ë“œ ìˆ˜", min_value=1, max_value=8, value=4)
        
        if num_heads > 1:
            head_dim = d_k // num_heads
            
            # í—¤ë“œë³„ë¡œ ë¶„í• 
            Q_heads = Q_torch.view(Q_torch.size(0), num_heads, head_dim)
            K_heads = K_torch.view(K_torch.size(0), num_heads, head_dim)
            V_heads = V_torch.view(V_torch.size(0), num_heads, head_dim)
            
            # ê° í—¤ë“œë³„ ì–´í…ì…˜ ê³„ì‚°
            head_outputs = []
            for i in range(num_heads):
                head_Q = Q_heads[:, i, :]
                head_K = K_heads[:, i, :]
                head_V = V_heads[:, i, :]
                
                head_scores = torch.matmul(head_Q, head_K.transpose(-2, -1)) / math.sqrt(head_dim)
                head_weights = torch.softmax(head_scores, dim=-1)
                head_context = torch.matmul(head_weights, head_V)
                head_outputs.append(head_context)
            
            # í—¤ë“œ ê²°ê³¼ ê²°í•©
            multihead_output = torch.cat(head_outputs, dim=-1)
            
            st.write(f"**ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ê²°ê³¼:**")
            st.write(f"- ì…ë ¥ shape: {Q_torch.shape}")
            st.write(f"- í—¤ë“œ ìˆ˜: {num_heads}")
            st.write(f"- í—¤ë“œ ì°¨ì›: {head_dim}")
            st.write(f"- ì¶œë ¥ shape: {multihead_output.shape}")
            
            # í—¤ë“œë³„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”
            st.markdown("**í—¤ë“œë³„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜:**")
            for i in range(num_heads):
                head_weights = torch.softmax(
                    torch.matmul(Q_heads[:, i, :], K_heads[:, i, :].transpose(-2, -1)) / math.sqrt(head_dim),
                    dim=-1
                )
                st.write(f"Head {i+1} weights shape: {head_weights.shape}")
        
        # ì„±ëŠ¥ ë¹„êµ
        st.markdown("### âš¡ ì„±ëŠ¥ ë¹„êµ")
        
        if st.button("NumPy vs PyTorch ì„±ëŠ¥ ë¹„êµ"):
            import time
            import numpy as np
            
            # NumPy êµ¬í˜„
            start_time = time.time()
            Q_np = src_E @ Wq
            K_np = src_E @ Wk
            V_np = src_E @ Wv
            scores_np = Q_np @ K_np.T / np.sqrt(d_k)
            weights_np = np.exp(scores_np - np.max(scores_np, axis=-1, keepdims=True))
            weights_np = weights_np / np.sum(weights_np, axis=-1, keepdims=True)
            context_np = weights_np @ V_np
            numpy_time = time.time() - start_time
            
            # PyTorch êµ¬í˜„
            start_time = time.time()
            Q_torch = torch.matmul(src_E_torch, Wq_torch)
            K_torch = torch.matmul(src_E_torch, Wk_torch)
            V_torch = torch.matmul(src_E_torch, Wv_torch)
            scores_torch = torch.matmul(Q_torch, K_torch.transpose(-2, -1)) / math.sqrt(d_k)
            weights_torch = torch.softmax(scores_torch, dim=-1)
            context_torch = torch.matmul(weights_torch, V_torch)
            pytorch_time = time.time() - start_time
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("NumPy ì‹¤í–‰ ì‹œê°„", f"{numpy_time:.6f}ì´ˆ")
            with col2:
                st.metric("PyTorch ì‹¤í–‰ ì‹œê°„", f"{pytorch_time:.6f}ì´ˆ")
            
            if numpy_time > pytorch_time:
                st.success("ğŸ‰ PyTorchê°€ ë” ë¹ ë¦…ë‹ˆë‹¤!")
            else:
                st.info("ğŸ“Š NumPyê°€ ë” ë¹ ë¦…ë‹ˆë‹¤.")
        
        # PyTorch ëª¨ë¸ ì €ì¥/ë¡œë“œ
        st.markdown("### ğŸ’¾ ëª¨ë¸ ì €ì¥/ë¡œë“œ")
        
        if st.button("PyTorch ëª¨ë¸ ì €ì¥"):
            # ê°„ë‹¨í•œ ì–´í…ì…˜ ëª¨ë¸ ìƒì„±
            attention_model = torch.nn.MultiheadAttention(
                embed_dim=d_k,
                num_heads=num_heads,
                batch_first=True
            )
            
            # ëª¨ë¸ ì €ì¥
            torch.save(attention_model.state_dict(), "attention_model.pth")
            st.success("âœ… ëª¨ë¸ì´ 'attention_model.pth'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        if st.button("PyTorch ëª¨ë¸ ë¡œë“œ"):
            try:
                # ëª¨ë¸ ë¡œë“œ
                attention_model = torch.nn.MultiheadAttention(
                    embed_dim=d_k,
                    num_heads=num_heads,
                    batch_first=True
                )
                attention_model.load_state_dict(torch.load("attention_model.pth"))
                st.success("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                st.write("**ë¡œë“œëœ ëª¨ë¸ ì •ë³´:**")
                st.write(f"- ì„ë² ë”© ì°¨ì›: {d_k}")
                st.write(f"- í—¤ë“œ ìˆ˜: {num_heads}")
                st.write(f"- ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in attention_model.parameters())}")
                
            except FileNotFoundError:
                st.error("âŒ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì €ì¥í•´ì£¼ì„¸ìš”.")
        
        # PyTorch ìµœì í™” íŒ
        st.markdown("### ğŸ’¡ PyTorch ìµœì í™” íŒ")
        
        with st.expander("PyTorch ì„±ëŠ¥ ìµœì í™” ë°©ë²•"):
            st.markdown("""
            **1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:**
            - `torch.no_grad()` ì‚¬ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë°©ì§€
            - ì ì ˆí•œ ë°ì´í„° íƒ€ì… ì„ íƒ (float32 vs float16)
            
            **2. ì—°ì‚° ìµœì í™”:**
            - `torch.matmul` ëŒ€ì‹  `@` ì—°ì‚°ì ì‚¬ìš©
            - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë²¡í„°í™” ì—°ì‚° í™œìš©
            
            **3. GPU í™œìš©:**
            - `tensor.cuda()` ë˜ëŠ” `tensor.to('cuda')` ì‚¬ìš©
            - `torch.cuda.empty_cache()`ë¡œ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            
            **4. ì»´íŒŒì¼ ìµœì í™”:**
            - PyTorch 2.0+ `torch.compile()` ì‚¬ìš©
            - JIT ì»´íŒŒì¼ë¡œ ì‹¤í–‰ ì†ë„ í–¥ìƒ
            """)
    
    else:
        st.warning("ë¨¼ì € 'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
