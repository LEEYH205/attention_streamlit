import streamlit as st
import google.generativeai as genai

def render_ai_chatbot(analyze, src_tokens, tgt_tokens, src_E, tgt_E, gemini_model):
    """AI ì±—ë´‡ íƒ­ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    
    st.subheader("ğŸ¤– Gemini AI ì±—ë´‡ê³¼ ëŒ€í™”í•´ë³´ì„¸ìš”!")
    
    # Gemini API ìƒíƒœ í™•ì¸
    if gemini_model:
        st.success("âœ… Gemini AIê°€ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        if analyze:
            # ë¶„ì„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
            analysis_context = f"""
            í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ë°ì´í„°:
            - ì†ŒìŠ¤ í† í° (í•œêµ­ì–´): {src_tokens}
            - íƒ€ê¹ƒ í† í° (ì˜ì–´): {tgt_tokens}
            - ì–´í…ì…˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
            """
            
            st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì§ˆë¬¸")
            
            # ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ë“¤
            predefined_questions = [
                "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
                "Q, K, Vì˜ ì—­í• ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ë§ˆìŠ¤í‚¹ì´ í•„ìš”í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "í˜„ì¬ ë¶„ì„ëœ ë¬¸ì¥ì˜ ì–´í…ì…˜ íŒ¨í„´ì„ í•´ì„í•´ì£¼ì„¸ìš”"
            ]
            
            st.markdown("**ğŸ’¡ ì¶”ì²œ ì§ˆë¬¸ë“¤:**")
            for i, question in enumerate(predefined_questions):
                if st.button(f"Q{i+1}: {question}", key=f"pre_q_{i}"):
                    with st.spinner("ğŸ¤” Gemini AIê°€ ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        try:
                            prompt = f"""
                            {analysis_context}
                            
                            ì§ˆë¬¸: {question}
                            
                            ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ êµìœ¡ì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
                            """
                            
                            response = gemini_model.generate_content(prompt)
                            st.markdown(f"**ğŸ¤– Gemini AI ë‹µë³€:**")
                            st.markdown(response.text)
                            
                        except Exception as e:
                            st.error(f"âŒ Gemini AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            st.markdown("---")
        
        # ììœ  ì§ˆë¬¸
        st.markdown("### ğŸ’¬ ììœ  ì§ˆë¬¸")
        user_input = st.text_area(
            "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”:",
            placeholder="ì˜ˆ: Transformerì™€ RNNì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            height=100
        )
        
        if st.button("ğŸ¤– Gemini AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°"):
            if user_input.strip():
                with st.spinner("ğŸ¤” Gemini AIê°€ ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    try:
                        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
                        context = ""
                        if analyze:
                            context = f"""
                            í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ë°ì´í„°:
                            - ì†ŒìŠ¤ í† í° (í•œêµ­ì–´): {src_tokens}
                            - íƒ€ê¹ƒ í† í° (ì˜ì–´): {tgt_tokens}
                            
                            """
                        
                        prompt = f"""
                        {context}
                        
                        ì§ˆë¬¸: {user_input}
                        
                        ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ëŒ€í•´ êµìœ¡ì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
                        ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                        """
                        
                        response = gemini_model.generate_content(prompt)
                        st.markdown(f"**ğŸ¤– Gemini AI ë‹µë³€:**")
                        st.markdown(response.text)
                        
                        # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì €ì¥
                        if 'chat_history' not in st.session_state:
                            st.session_state.chat_history = []
                        
                        st.session_state.chat_history.append({
                            'user': user_input,
                            'ai': response.text
                        })
                        
                    except Exception as e:
                        st.error(f"âŒ Gemini AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            else:
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
        if 'chat_history' in st.session_state and st.session_state.chat_history:
            st.markdown("### ğŸ“ ëŒ€í™” íˆìŠ¤í† ë¦¬")
            
            for i, chat in enumerate(st.session_state.chat_history):
                with st.expander(f"ëŒ€í™” {i+1}", expanded=False):
                    st.markdown(f"**ğŸ‘¤ ì‚¬ìš©ì:** {chat['user']}")
                    st.markdown(f"**ğŸ¤– Gemini AI:** {chat['ai']}")
            
            if st.button("ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚­ì œ"):
                st.session_state.chat_history = []
                st.rerun()
        
        # ê³ ê¸‰ ê¸°ëŠ¥
        st.markdown("### ğŸš€ ê³ ê¸‰ ê¸°ëŠ¥")
        
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‹œë®¬ë ˆì´ì…˜
        if st.checkbox("ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì‹œë®¬ë ˆì´ì…˜"):
            st.markdown("**ê°„ë‹¨í•œ ì–´í…ì…˜ ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜:**")
            
            # ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥
            user_sentence = st.text_input("ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:", value="ë‚˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì„ ê³µë¶€í•©ë‹ˆë‹¤")
            
            if user_sentence:
                tokens = user_sentence.split()
                st.write(f"**í† í°í™” ê²°ê³¼:** {tokens}")
                
                # ê°„ë‹¨í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ì˜ˆì‹œ)
                import numpy as np
                attention_weights = np.random.rand(len(tokens), len(tokens))
                attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)
                
                st.write("**ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ëœë¤ ì˜ˆì‹œ):**")
                st.dataframe(attention_weights, 
                           index=[f"Query: {token}" for token in tokens],
                           columns=[f"Key: {token}" for token in tokens])
        
        # ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„ ë„ìš°ë¯¸
        if st.checkbox("ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„ ë„ìš°ë¯¸"):
            st.markdown("**ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ ë„êµ¬:**")
            
            pattern_type = st.selectbox(
                "ë¶„ì„í•  ì–´í…ì…˜ íŒ¨í„´ ìœ í˜•:",
                ["Self-Attention", "Cross-Attention", "Masked Attention"]
            )
            
            if st.button("íŒ¨í„´ ë¶„ì„ ìš”ì²­"):
                prompt = f"""
                {pattern_type}ì— ëŒ€í•´ ë‹¤ìŒì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:
                1. ê¸°ë³¸ ê°œë…ê³¼ ì‘ë™ ì›ë¦¬
                2. ì–¸ì œ ì‚¬ìš©ë˜ëŠ”ì§€
                3. ì¥ë‹¨ì 
                4. ì‹¤ì œ ì˜ˆì‹œ
                
                í•œêµ­ì–´ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                """
                
                with st.spinner("íŒ¨í„´ ë¶„ì„ ì¤‘..."):
                    try:
                        response = gemini_model.generate_content(prompt)
                        st.markdown(f"**ğŸ” {pattern_type} ë¶„ì„ ê²°ê³¼:**")
                        st.markdown(response.text)
                    except Exception as e:
                        st.error(f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
    
    else:
        st.warning("âš ï¸ Gemini APIê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.markdown("""
        **API í‚¤ ì„¤ì • ë°©ë²•:**
        1. ì‚¬ì´ë“œë°”ì— Google Gemini API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”
        2. ë˜ëŠ” `config.env` íŒŒì¼ì— `GOOGLE_API_KEY`ë¥¼ ì„¤ì •í•˜ì„¸ìš”
        3. API í‚¤ëŠ” [Google AI Studio](https://aistudio.google.com/app/apikey)ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        
        **ëŒ€ì•ˆ:**
        - ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - ë˜ëŠ” ë‹¤ë¥¸ AI ì„œë¹„ìŠ¤ë¥¼ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        """)
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ì±—ë´‡ (fallback)
        st.markdown("### ğŸ” ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ì±—ë´‡")
        
        user_input = st.text_input("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: attention, transformer, softmax):")
        
        if user_input:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ
            responses = {
                "attention": "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ íŠ¹ì • ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
                "transformer": "TransformerëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.",
                "softmax": "SoftmaxëŠ” ë²¡í„°ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.",
                "qkv": "Q(Query), K(Key), V(Value)ëŠ” ì–´í…ì…˜ ê³„ì‚°ì˜ í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.",
                "multihead": "ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì€ ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤."
            }
            
            response = responses.get(user_input.lower(), "í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.info(f"**ë‹µë³€:** {response}")
