
# 🎯 Attention 교육용 Streamlit 데모

**Transformer의 핵심 메커니즘인 Attention을 단계별로 학습할 수 있는 대화형 교육 도구입니다.**

## ✨ 주요 기능

### 1. **단계별 어텐션** 📚
- **Q, K, V 직관적 설명**: 도서관 비유를 통한 이해하기 쉬운 개념 설명
- **인코더 Self-Attention**: 한국어 문장 내 단어 간 관계 학습
- **크로스 어텐션**: 한국어→영어 번역 시 참고 관계 시각화
- **마스크드 셀프 어텐션**: 디코더에서 미래 토큰을 보지 못하게 하는 원리
- **단계별 계산 과정**: 선형 변환 → 스코어 계산 → Softmax → 가중합

### 2. **멀티헤드 어텐션** 🔍
- **4개 헤드 병렬 처리**: 서로 다른 관점에서 어텐션 패턴 학습
- **분할-병렬-결합 과정**: 멀티헤드 어텐션의 전체 과정 시각화
- **각 헤드별 어텐션 패턴**: 다양한 관계(문법적, 의미적) 동시 파악

### 3. **어텐션 지도 상세 분석** 📊
- **어텐션 매트릭스 히트맵**: 직관적인 시각화
- **원본단어별 총 어텐션**: 각 한국어 단어가 받는 총 주목도
- **번역 단어별 어텐션 분산**: 영어 단어의 어텐션 집중/분산 패턴
- **어텐션 엔트로피**: 각 타깃 단어의 어텐션 분산도 측정
- **통계 요약**: 평균, 최대, 최소, 표준편차 등 종합 분석

### 4. **임베딩 분석** 🧠
- **코사인 유사도**: 단어 간 의미적 유사성 측정
- **PCA 2D 분포**: 고차원 임베딩을 2D로 압축하여 시각화
- **소스/타깃 구분**: 한국어와 영어 단어의 임베딩 공간 분포

### 5. **PyTorch 구현** ⚡
- **실제 PyTorch 텐서**: numpy 대신 PyTorch로 어텐션 계산
- **형상 검증**: 각 단계별 텐서 차원 확인
- **교육용 실습**: 실제 딥러닝 프레임워크 사용 경험

### 6. **AI 챗봇 (개선됨)** 🤖
- **분석 결과 기반 질문**: 실제 계산된 어텐션 데이터 활용
- **동적 질문 시스템**: 
  - 특정 단어의 어텐션 패턴
  - 최고 어텐션 스코어 쌍
  - 엔트로피 기반 분산도 분석
- **확장된 지식베이스**: Q, K, V, 멀티헤드, 마스킹 등 추가 개념

## 🚀 설치 & 실행

### 1. 가상환경 생성 및 활성화
```bash
# 가상환경 생성
python -m venv .venv

# 가상환경 활성화
source .venv/bin/activate  # macOS/Linux
# 또는
.venv\Scripts\activate     # Windows
```

### 2. 의존성 설치
```bash
pip install -r requirements.txt
```

### 3. 앱 실행
```bash
streamlit run app.py
```

### 4. 브라우저에서 확인
- **로컬**: http://localhost:8501
- **네트워크**: http://[your-ip]:8501

## 📖 사용 방법

### 1. **기본 설정**
- 사이드바에서 한국어 문장 입력 (예: "나는 밥을 먹었어")
- 영어 번역문 입력 (예: "I ate a meal")
- "분석 시작" 버튼 클릭

### 2. **탭별 학습**
- **단계별 어텐션**: 기본 개념부터 단계별 학습
- **멀티헤드 어텐션**: 여러 관점의 어텐션 패턴
- **어텐션 지도**: 상세한 분석 및 통계
- **임베딩 분석**: 단어 간 유사성 및 분포
- **PyTorch 구현**: 실제 프레임워크 사용
- **AI 챗봇**: 분석 결과에 대한 질문

### 3. **고급 옵션**
- **수식 표시**: 수학적 공식 표시/숨김
- **계산 과정**: 각 단계별 중간 결과 표시/숨김

## 🔧 기술적 특징

### **한글 폰트 지원**
- macOS: AppleGothic
- Windows: Malgun Gothic  
- Linux: DejaVu Sans
- 마이너스 기호 깨짐 방지

### **교육용 구현**
- **해시 기반 임베딩**: 결정적이고 재현 가능한 결과
- **사인-코사인 위치인코딩**: 논문과 동일한 방식
- **고정된 가중치**: 일관된 학습 경험 제공

## 📚 학습 효과

### **초보자를 위한 설계**
- **직관적 비유**: 도서관 검색으로 Q, K, V 이해
- **단계별 설명**: 복잡한 개념을 작은 단위로 분해
- **시각적 피드백**: 히트맵과 그래프로 직관적 이해
- **상호작용**: 버튼 클릭으로 동적 학습

### **고급 개념 포함**
- **마스킹**: 실제 Transformer 디코더의 핵심 메커니즘
- **멀티헤드**: 다양한 관계를 동시에 학습하는 방법
- **엔트로피**: 어텐션 패턴의 복잡성 측정

## ⚠️ 주의사항

- **교육용 목적**: 실제 번역 모델의 내부 가중치와는 다름
- **해시 기반 임베딩**: 의미적 유사성이 아닌 결정적 매핑
- **고정된 가중치**: 학습 과정에서 가중치가 업데이트되지 않음
- **한글 폰트**: 시스템별로 적절한 폰트 자동 선택

## 🤝 기여하기

이 프로젝트는 교육 목적으로 만들어졌습니다. 개선 제안이나 버그 리포트는 언제든 환영합니다!

## 📄 라이선스

교육 및 연구 목적으로 자유롭게 사용하실 수 있습니다.

---

**🎯 Attention 메커니즘의 모든 것을 한 번에 배우세요!**
