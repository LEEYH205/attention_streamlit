
# -*- coding: utf-8 -*-
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
from dataclasses import dataclass
import math

# í•œê¸€ í°íŠ¸ ì„¤ì •
import matplotlib.font_manager as fm
import platform

# ìš´ì˜ì²´ì œë³„ í•œê¸€ í°íŠ¸ ì„¤ì •
if platform.system() == 'Darwin':  # macOS
    plt.rcParams['font.family'] = 'AppleGothic'
elif platform.system() == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
else:  # Linux
    plt.rcParams['font.family'] = 'DejaVu Sans'
    
# ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False

st.set_page_config(page_title="Attention êµìœ¡ìš© ë°ëª¨", layout="wide")

# -----------------------------
# Utilities
# -----------------------------
def tokenize(text: str):
    """ê°„ë‹¨ í† í¬ë‚˜ì´ì €: ê³µë°±/êµ¬ë‘ì  ê¸°ì¤€ ë¶„ë¦¬"""
    import re
    # ë‹¨ì–´, ìˆ«ì, ì•„í¬ìŠ¤íŠ¸ë¡œí”¼/í•˜ì´í”ˆì„ í¬í•¨í•˜ëŠ” í† í° + ê·¸ ì™¸ êµ¬ë‘ì 
    tokens = re.findall(r"[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿0-9ê°€-í£]+(?:['-][A-Za-z0-9ê°€-í£]+)?|[^\sA-Za-z0-9ê°€-í£]", text.strip())
    return tokens if tokens else ["<empty>"]

def sinusoidal_positional_encoding(n: int, d: int):
    """ë…¼ë¬¸ì‹ ì‚¬ì¸-ì½”ì‚¬ì¸ ìœ„ì¹˜ì¸ì½”ë”©"""
    pe = np.zeros((n, d), dtype=np.float64)
    position = np.arange(n)[:, None]
    div_term = np.exp(np.arange(0, d, 2) * -(np.log(10000.0) / d))
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    return pe

def hash_vec(token: str, dim: int = 32) -> np.ndarray:
    """í† í°ë³„ ê²°ì •ì  í•´ì‹œ ì„ë² ë”©(ì‹œì—°ìš©)"""
    rs = np.random.RandomState(abs(hash(token)) % (2**32))
    return rs.normal(0, 1, size=dim)

def softmax(x, axis=-1):
    x = x - np.max(x, axis=axis, keepdims=True)
    ex = np.exp(x)
    return ex / np.sum(ex, axis=axis, keepdims=True)

def calculate_entropy(attention_weights):
    """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
    # 0ì— ê°€ê¹Œìš´ ê°’ë“¤ì„ ì‘ì€ ì–‘ìˆ˜ë¡œ ëŒ€ì²´
    eps = 1e-10
    weights = np.maximum(attention_weights, eps)
    # ì •ê·œí™”
    weights = weights / np.sum(weights, axis=-1, keepdims=True)
    # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: -sum(p * log(p))
    entropy = -np.sum(weights * np.log(weights), axis=-1)
    return np.mean(entropy)  # í‰ê·  ì—”íŠ¸ë¡œí”¼ ë°˜í™˜

def scaled_dot_product_attention(Q, K, V, mask=None):
    dk = Q.shape[-1]
    scores = Q @ K.T / np.sqrt(dk)  # [tgt, src]
    if mask is not None:
        scores = np.where(mask, scores, -1e9)
    weights = softmax(scores, axis=-1)
    context = weights @ V
    return context, weights, scores

def linear_projection(X, W, b=None):
    Y = X @ W
    if b is not None:
        Y += b
    return Y

def np_to_df(mat, row_idx=None, col_idx=None, floatfmt=6):
    import pandas as pd
    df = pd.DataFrame(np.round(mat.astype(float), floatfmt))
    
    # ì¤‘ë³µëœ ì¸ë±ìŠ¤/ì»¬ëŸ¼ ì´ë¦„ ë¬¸ì œ í•´ê²°
    if row_idx is not None:
        # ì¤‘ë³µëœ ê°’ì´ ìˆìœ¼ë©´ _ìˆ«ì ì¶”ê°€
        unique_row_idx = []
        seen = {}
        for i, val in enumerate(row_idx):
            if val in seen:
                seen[val] += 1
                unique_row_idx.append(f"{val}_{seen[val]}")
            else:
                seen[val] = 0
                unique_row_idx.append(val)
        df.index = unique_row_idx
    
    if col_idx is not None:
        # ì¤‘ë³µëœ ê°’ì´ ìˆìœ¼ë©´ _ìˆ«ì ì¶”ê°€
        unique_col_idx = []
        seen = {}
        for i, val in enumerate(col_idx):
            if val in seen:
                seen[val] += 1
                unique_col_idx.append(f"{val}_{seen[val]}")
            else:
                seen[val] = 0
                unique_col_idx.append(val)
        df.columns = unique_col_idx
    
    return df

def plot_heatmap(W, xticks, yticks, title=""):
    fig, ax = plt.subplots()
    im = ax.imshow(W)  # ìƒ‰ìƒì€ ê¸°ë³¸ê°’ ì‚¬ìš©(ê·œì •: íŠ¹ì • ìƒ‰ìƒ ì§€ì • ê¸ˆì§€)
    ax.set_xticks(np.arange(len(xticks)))
    ax.set_yticks(np.arange(len(yticks)))
    ax.set_xticklabels(xticks, rotation=45, ha="right")
    ax.set_yticklabels(yticks)
    ax.set_title(title, fontsize=12)
    # ê°’ ì£¼ì„(ì¹¸ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒëµ)
    if W.shape[0]*W.shape[1] <= 400:
        for i in range(W.shape[0]):
            for j in range(W.shape[1]):
                ax.text(j, i, f"{W[i, j]:.2f}", ha="center", va="center", fontsize=8)
    st.pyplot(fig)

def pca_2d(X, k=2):
    """numpy SVD ê¸°ë°˜ ê°„ë‹¨ PCA"""
    Xc = X - X.mean(axis=0, keepdims=True)
    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)
    Z = Xc @ Vt.T[:, :k]
    return Z

# -----------------------------
# Sidebar (ì…ë ¥/ì˜µì…˜/ë²„íŠ¼)
# -----------------------------
st.sidebar.header("ì…ë ¥ & ì˜µì…˜")
src_text = st.sidebar.text_area("ì›ë³¸ë¬¸ì¥ (í•œêµ­ì–´)", "ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆì–´")
tgt_text = st.sidebar.text_area("ë²ˆì—­ë¬¸ì¥ (ì˜ì–´)", "I ate a meal")
show_formula = st.sidebar.checkbox("ìˆ˜ì‹ í‘œì‹œ", value=True)
show_steps = st.sidebar.checkbox("ê³„ì‚° ê³¼ì •", value=True)
analyze = st.sidebar.button("ë¶„ì„ ì‹œì‘")

st.title("ğŸ¯ Attention êµìœ¡ìš© Streamlit ë°ëª¨")
st.caption("ë‹¨ê³„ë³„ ì–´í…ì…˜ Â· ë©€í‹°í—¤ë“œ ì–´í…ì…˜ Â· ì–´í…ì…˜ ì§€ë„ Â· ì„ë² ë”© ë¶„ì„ Â· PyTorch êµ¬í˜„ Â· AI ì±—ë´‡")

tabs = st.tabs(["ğŸ§­ í•™ìŠµ ê°€ì´ë“œ(ì´ˆë³´ììš©)", "ğŸ”¬ ë‹¨ê³„ë³„ ì–´í…ì…˜(ì‹¤í—˜ì‹¤)", "ğŸ§© ë©€í‹°í—¤ë“œ ì‹œê°í™”", "â›” ë§ˆìŠ¤í‚¹ & ì¸ê³¼ì„±", "ì–´í…ì…˜ ì§€ë„", "ì„ë² ë”© ë¶„ì„", "PyTorch êµ¬í˜„", "AI ì±—ë´‡", "ğŸ“ í€´ì¦ˆ", "ğŸ“š ìš©ì–´ì‚¬ì „"])

# ê³µí†µ ì „ì²˜ë¦¬
src_tokens = tokenize(src_text)
tgt_tokens = tokenize(tgt_text)

dim = 32  # ì„ë² ë”© ì°¨ì›(ì‹œì—°ìš©)
src_E = np.stack([hash_vec(t, dim) for t in src_tokens])
tgt_E = np.stack([hash_vec(t, dim) for t in tgt_tokens])
src_E = src_E + sinusoidal_positional_encoding(len(src_tokens), dim)
tgt_E = tgt_E + sinusoidal_positional_encoding(len(tgt_tokens), dim)

# ì„ í˜• ë³€í™˜ ê°€ì¤‘ì¹˜(ì‹œì—°ìš© ê³ ì • ë‚œìˆ˜)
np.random.seed(42)
Wq = np.random.normal(0, 1 / np.sqrt(dim), size=(dim, dim))
Wk = np.random.normal(0, 1 / np.sqrt(dim), size=(dim, dim))
Wv = np.random.normal(0, 1 / np.sqrt(dim), size=(dim, dim))

# --------------------------------------------------
# íƒ­ 1: í•™ìŠµ ê°€ì´ë“œ(ì´ˆë³´ììš©)
# --------------------------------------------------
with tabs[0]:
    st.subheader("ğŸ¯ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ í•™ìŠµ ê°€ì´ë“œ")
    st.markdown("""
    ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•´ë³´ì„¸ìš”. ê° ë‹¨ê³„ë¥¼ í´ë¦­í•˜ë©´ í•´ë‹¹ ë‚´ìš©ì„ ìì„¸íˆ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    # ë‹¨ê³„ë³„ í•™ìŠµ ê°€ì´ë“œ
    step = st.selectbox("í•™ìŠµ ë‹¨ê³„ ì„ íƒ", 
                        ["0. ê°œìš”", "1. í† í°í™” & ì„ë² ë”©", "2. ì„ í˜•ì‚¬ì˜(Q,K,V)", 
                         "3. ìœ ì‚¬ë„(QKáµ€)", "4. ìŠ¤ì¼€ì¼ë§(/âˆšdâ‚–)", "5. ì†Œí”„íŠ¸ë§¥ìŠ¤(ê°€ì¤‘ì¹˜)", "6. ê°€ì¤‘í•©(ì»¨í…ìŠ¤íŠ¸)"])
    
    if step == "0. ê°œìš”":
        st.markdown("""
        ### ğŸ¯ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ë€?
        
        **ì–´í…ì…˜(Attention)**ì€ ë”¥ëŸ¬ë‹ì—ì„œ ì…ë ¥ì˜ íŠ¹ì • ë¶€ë¶„ì— ì§‘ì¤‘í•˜ì—¬ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.
        
        **í•µì‹¬ ì•„ì´ë””ì–´**: 
        - ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ê³ ë ¤
        - ê° ìœ„ì¹˜ì˜ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ ê³„ì‚°
        - ì¤‘ìš”í•œ ì •ë³´ì— ë” ì§‘ì¤‘í•˜ì—¬ ì²˜ë¦¬
        
        **íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œì˜ ì—­í• **:
        1. **Self-Attention**: ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ì˜ ê´€ê³„ í•™ìŠµ
        2. **Cross-Attention**: ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê°„ì˜ ê´€ê³„ í•™ìŠµ
        3. **Masked Attention**: ë¯¸ë˜ ì •ë³´ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ì œì•½
        """)
        
    elif step == "1. í† í°í™” & ì„ë² ë”©":
        st.markdown("""
        ### ğŸ“ 1ë‹¨ê³„: í† í°í™” & ì„ë² ë”©
        
        **í† í°í™”(Tokenization)**:
        - í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„(í† í°)ë¡œ ë¶„ë¦¬
        - ì˜ˆ: "ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆì–´" â†’ ["ë‚˜ëŠ”", "ë°¥ì„", "ë¨¹ì—ˆì–´"]
        
        **ì„ë² ë”©(Embedding)**:
        - ê° í† í°ì„ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
        - ì˜ë¯¸ì  ì •ë³´ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„
        - ìœ„ì¹˜ ì •ë³´ë„ í•¨ê»˜ ì¸ì½”ë”© (Positional Encoding)
        """)
        
        if analyze:
            st.write("**í˜„ì¬ ì˜ˆì‹œ**:")
            col1, col2 = st.columns(2)
            with col1:
                st.write("ì†ŒìŠ¤ í† í°:", src_tokens)
                st.write("ì†ŒìŠ¤ ì„ë² ë”© shape:", src_E.shape)
            with col2:
                st.write("íƒ€ê¹ƒ í† í°:", tgt_tokens)
                st.write("íƒ€ê¹ƒ ì„ë² ë”© shape:", tgt_E.shape)
    
    elif step == "2. ì„ í˜•ì‚¬ì˜(Q,K,V)":
        st.markdown("""
        ### ğŸ”„ 2ë‹¨ê³„: ì„ í˜•ì‚¬ì˜ìœ¼ë¡œ Q, K, V ìƒì„±
        
        **Q(Query), K(Key), V(Value)ì˜ ì—­í• **:
        
        - **Query (Q)**: "ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?" - í˜„ì¬ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€
        - **Key (K)**: "ë¬´ì—‡ì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ê°€?" - ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì´ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ì˜ íŠ¹ì§•
        - **Value (V)**: "ì‹¤ì œ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€?" - ê° ë‹¨ì–´ê°€ ì‹¤ì œë¡œ ë‹´ê³  ìˆëŠ” ì˜ë¯¸ ì •ë³´
        
        **ìˆ˜ì‹**: `Q = XW_Q`, `K = XW_K`, `V = XW_V`
        """)
        
        if analyze:
            Qs = linear_projection(src_E, Wq)
            Ks = linear_projection(src_E, Wk)
            Vs = linear_projection(src_E, Wv)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("**Query (Q)**")
                st.write("shape:", Qs.shape)
                st.dataframe(np_to_df(Qs, row_idx=src_tokens))
            with col2:
                st.markdown("**Key (K)**")
                st.write("shape:", Ks.shape)
                st.dataframe(np_to_df(Ks, row_idx=src_tokens))
            with col3:
                st.markdown("**Value (V)**")
                st.write("shape:", Vs.shape)
                st.dataframe(np_to_df(Vs, row_idx=src_tokens))
    
    elif step == "3. ìœ ì‚¬ë„(QKáµ€)":
        st.markdown("""
        ### ğŸ” 3ë‹¨ê³„: ìœ ì‚¬ë„ ê³„ì‚° (QKáµ€)
        
        **ëª©ì **: ê° Queryì™€ ëª¨ë“  Key ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°
        
        **ìˆ˜ì‹**: `Scores = QK^T`
        
        **ì˜ë¯¸**: 
        - ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ Query-Key ìŒì´ ë” ê´€ë ¨ì„±ì´ ë†’ìŒ
        - ì–´í…ì…˜ì„ ì¤„ ë‹¨ì–´ë¥¼ ê²°ì •í•˜ëŠ” í•µì‹¬ ë‹¨ê³„
        """)
        
        if analyze:
            Qs = linear_projection(src_E, Wq)
            Ks = linear_projection(src_E, Wk)
            Ss = Qs @ Ks.T
            
            st.write("**Score Matrix (QKáµ€)**")
            st.write("shape:", Ss.shape)
            st.dataframe(np_to_df(Ss, row_idx=src_tokens, col_idx=src_tokens))
            
            st.markdown("**í•´ì„**:")
            st.markdown("- í–‰: Query (ì–´í…ì…˜ì„ ì£¼ëŠ” ë‹¨ì–´)")
            st.markdown("- ì—´: Key (ì–´í…ì…˜ì„ ë°›ëŠ” ë‹¨ì–´)")
            st.markdown("- ê°’ì´ í´ìˆ˜ë¡ ë” ê´€ë ¨ì„±ì´ ë†’ìŒ")
    
    elif step == "4. ìŠ¤ì¼€ì¼ë§(/âˆšdâ‚–)":
        st.markdown("""
        ### ğŸ“ 4ë‹¨ê³„: ìŠ¤ì¼€ì¼ë§ (/âˆšdâ‚–)
        
        **ëª©ì **: ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê¸°
        
        **ìˆ˜ì‹**: `Scores = QK^T / âˆšd_k`
        
        **ì™œ í•„ìš”í•œê°€?**:
        - d_kê°€ í´ ë•Œ QK^T ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ì„œ softmaxì—ì„œ ê¸°ìš¸ê¸° ì†Œì‹¤ ë°œìƒ
        - âˆšd_kë¡œ ë‚˜ëˆ„ì–´ ë¶„ì‚°ì„ 1ë¡œ ì •ê·œí™”
        - í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
        """)
        
        if analyze:
            Qs = linear_projection(src_E, Wq)
            Ks = linear_projection(src_E, Wk)
            dk = Qs.shape[-1]
            
            # ìŠ¤ì¼€ì¼ë§ ì „í›„ ë¹„êµ
            Ss_raw = Qs @ Ks.T
            Ss_scaled = Ss_raw / np.sqrt(dk)
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**ìŠ¤ì¼€ì¼ë§ ì „**")
                st.write("í‘œì¤€í¸ì°¨:", np.std(Ss_raw))
                st.dataframe(np_to_df(Ss_raw, row_idx=src_tokens, col_idx=src_tokens))
            with col2:
                st.markdown("**ìŠ¤ì¼€ì¼ë§ í›„**")
                st.write("í‘œì¤€í¸ì°¨:", np.std(Ss_scaled))
                st.dataframe(np_to_df(Ss_scaled, row_idx=src_tokens, col_idx=src_tokens))
    
    elif step == "5. ì†Œí”„íŠ¸ë§¥ìŠ¤(ê°€ì¤‘ì¹˜)":
        st.markdown("""
        ### ğŸ² 5ë‹¨ê³„: Softmaxë¡œ ê°€ì¤‘ì¹˜ ìƒì„±
        
        **ëª©ì **: ìŠ¤ì½”ì–´ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
        
        **ìˆ˜ì‹**: `Weights = softmax(Scores)`
        
        **íŠ¹ì§•**:
        - ê° í–‰ì˜ í•©ì´ 1ì´ ë¨
        - ê°’ì´ í´ìˆ˜ë¡ ë” ë†’ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜
        - ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ `softmax(x - max(x))` ì‚¬ìš©
        """)
        
        if analyze:
            Qs = linear_projection(src_E, Wq)
            Ks = linear_projection(src_E, Wk)
            dk = Qs.shape[-1]
            Ss = Qs @ Ks.T / np.sqrt(dk)
            Ws = softmax(Ss, axis=-1)
            
            st.write("**Softmax Weights**")
            st.write("shape:", Ws.shape)
            st.dataframe(np_to_df(Ws, row_idx=src_tokens, col_idx=src_tokens))
            
            # ìˆ˜ì¹˜ ì•ˆì •ì„± ë°ëª¨
            st.markdown("**ìˆ˜ì¹˜ ì•ˆì •ì„± ë°ëª¨**:")
            st.markdown("`softmax(x - max(x))`ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ :")
            
            # í° ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            test_scores = np.array([1000, 1001, 1002])
            st.write("í…ŒìŠ¤íŠ¸ ìŠ¤ì½”ì–´:", test_scores)
            
            # ì¼ë°˜ì ì¸ softmax (ì˜¤ë²„í”Œë¡œìš° ìœ„í—˜)
            try:
                exp_scores = np.exp(test_scores)
                softmax_normal = exp_scores / np.sum(exp_scores)
                st.write("ì¼ë°˜ softmax:", softmax_normal)
            except:
                st.error("ì˜¤ë²„í”Œë¡œìš° ë°œìƒ!")
            
            # ì•ˆì •ì ì¸ softmax
            stable_scores = test_scores - np.max(test_scores)
            exp_stable = np.exp(stable_scores)
            softmax_stable = exp_stable / np.sum(exp_stable)
            st.write("ì•ˆì •ì  softmax:", softmax_stable)
    
    elif step == "6. ê°€ì¤‘í•©(ì»¨í…ìŠ¤íŠ¸)":
        st.markdown("""
        ### ğŸ¯ 6ë‹¨ê³„: ê°€ì¤‘í•©ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ë²¡í„° ìƒì„±
        
        **ëª©ì **: ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Valueì˜ ê°€ì¤‘ í‰ê·  ê³„ì‚°
        
        **ìˆ˜ì‹**: `Context = Weights Ã— V`
        
        **ê²°ê³¼**: 
        - ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ ì •ë³´ë¥¼ ì¢…í•©í•œ ìƒˆë¡œìš´ í‘œí˜„
        - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ìµœì¢… ì¶œë ¥
        - ë‹¤ìŒ ë ˆì´ì–´ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
        """)
        
        if analyze:
            Qs = linear_projection(src_E, Wq)
            Ks = linear_projection(src_E, Wk)
            Vs = linear_projection(src_E, Wv)
            dk = Qs.shape[-1]
            Ss = Qs @ Ks.T / np.sqrt(dk)
            Ws = softmax(Ss, axis=-1)
            Cs = Ws @ Vs
            
            st.write("**Context Vector (ìµœì¢… ì¶œë ¥)**")
            st.write("shape:", Cs.shape)
            st.dataframe(np_to_df(Cs, row_idx=src_tokens))
            
            st.markdown("**í•´ì„**:")
            st.markdown("- ê° í–‰: í•´ë‹¹ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ ì •ë³´ë¥¼ ì¢…í•©í•œ í‘œí˜„")
            st.markdown("- ì›ë³¸ ì„ë² ë”©ê³¼ ë‹¤ë¥¸ ìƒˆë¡œìš´ ë²¡í„°")
            st.markdown("- ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ ê²°ê³¼ë¬¼")

# --------------------------------------------------
# íƒ­ 2: ë‹¨ê³„ë³„ ì–´í…ì…˜(ì‹¤í—˜ì‹¤)
# --------------------------------------------------
with tabs[1]:
    st.subheader("ğŸ”¬ ì–´í…ì…˜ ì‹¤í—˜ì‹¤")
    st.markdown("""
    ë‹¤ì–‘í•œ ì–´í…ì…˜ ìœ í˜•ì„ ì‹¤í—˜í•´ë³´ì„¸ìš”. ê° ìœ í˜•ì„ ì„ íƒí•˜ê³  ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    # ì–´í…ì…˜ ìœ í˜• ì„ íƒ
    attention_type = st.radio(
        "ì–´í…ì…˜ ìœ í˜• ì„ íƒ",
        ["ì¸ì½”ë” Self-Attention", "ë””ì½”ë” Masked Self-Attention", "í¬ë¡œìŠ¤ Attention"],
        help="ê° ìœ í˜•ì˜ íŠ¹ì§•ì„ í™•ì¸í•´ë³´ì„¸ìš”"
    )
    
    if show_formula:
        st.markdown("**ê³µì‹**")
        st.latex(r"Q = XW_Q,\ K = XW_K,\ V = XW_V")
        st.latex(r"\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V")
        st.caption("â€» ì—¬ê¸°ì„œëŠ” êµìœ¡ìš©ìœ¼ë¡œ ì†ŒìŠ¤/íƒ€ê¹ƒ ì„ë² ë”©ì— ê°™ì€ ì°¨ì›/ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    st.markdown("**í† í°**")
    c1, c2 = st.columns(2)
    with c1:
        st.write("ì†ŒìŠ¤ í† í°:", src_tokens)
    with c2:
        st.write("íƒ€ê¹ƒ í† í°:", tgt_tokens)

    if analyze:
        if attention_type == "ì¸ì½”ë” Self-Attention":
            st.markdown("### ğŸ” ì¸ì½”ë” Self-Attention (ì†ŒìŠ¤â†’ì†ŒìŠ¤)")
            st.markdown("**ëª©ì **: í•œêµ­ì–´ ë¬¸ì¥ ë‚´ì—ì„œ ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ í•™ìŠµ")
            st.markdown("**íŠ¹ì§•**: ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆìŒ (ì–‘ë°©í–¥)")
            
            # A-1. ì„ í˜• ë³€í™˜ (Query, Key, Value ìƒì„±)
            st.markdown("#### A-1. ì„ í˜• ë³€í™˜: Query, Key, Value ìƒì„±")
            st.markdown("ê° ë‹¨ì–´ ì„ë² ë”©ì„ Query, Key, Valueë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            
            Qs = linear_projection(src_E, Wq)
            Ks = linear_projection(src_E, Wk)
            Vs = linear_projection(src_E, Wv)
            
            if show_steps:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown("**Query (Q)**")
                    st.write("shape:", Qs.shape)
                    st.dataframe(np_to_df(Qs, row_idx=src_tokens))
                with col2:
                    st.markdown("**Key (K)**")
                    st.write("shape:", Ks.shape)
                    st.dataframe(np_to_df(Ks, row_idx=src_tokens))
                with col3:
                    st.markdown("**Value (V)**")
                    st.write("shape:", Vs.shape)
                    st.dataframe(np_to_df(Vs, row_idx=src_tokens))
            
            # A-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
            st.markdown("#### A-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°: QKáµ€/âˆšd")
            st.markdown("ê° ë‹¨ì–´ ìŒ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.")
            
            dk = Qs.shape[-1]
            Ss = Qs @ Ks.T / np.sqrt(dk)
            
            if show_steps:
                st.write("**Score Matrix (QKáµ€/âˆšd)**")
                st.write("shape:", Ss.shape)
                st.dataframe(np_to_df(Ss, row_idx=src_tokens, col_idx=src_tokens))
                st.markdown("**í•´ì„**: ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ë‹¨ì–´ ìŒì´ ë” ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
            
            # A-3. Softmax ì ìš©
            st.markdown("#### A-3. Softmax ì ìš©: í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜")
            st.markdown("ìŠ¤ì½”ì–´ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•˜ì—¬ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.")
            
            Ws = softmax(Ss, axis=-1)
            
            if show_steps:
                st.write("**Softmax Weights**")
                st.write("shape:", Ws.shape)
                st.dataframe(np_to_df(Ws, row_idx=src_tokens, col_idx=src_tokens))
                st.markdown("**í•´ì„**: ê° í–‰ì˜ í•©ì´ 1ì´ ë˜ë©°, ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ë‹¨ì–´ì— ë” ì§‘ì¤‘í•©ë‹ˆë‹¤.")
            
            # A-4. ìµœì¢… ì¶œë ¥ ê³„ì‚°
            st.markdown("#### A-4. ìµœì¢… ì¶œë ¥ ê³„ì‚°: Weighted Sum")
            st.markdown("ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Valueì˜ ê°€ì¤‘ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")
            
            Cs = Ws @ Vs
            
            if show_steps:
                st.write("**Context Vector (ìµœì¢… ì¶œë ¥)**")
                st.write("shape:", Cs.shape)
                st.dataframe(np_to_df(Cs, row_idx=src_tokens))
                st.markdown("**í•´ì„**: ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ ì •ë³´ë¥¼ ì¢…í•©í•œ ìƒˆë¡œìš´ í‘œí˜„ì…ë‹ˆë‹¤.")
            
            # A-5. ì‹œê°í™”
            st.markdown("#### A-5. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
            plot_heatmap(Ws, xticks=src_tokens, yticks=src_tokens, title="ì¸ì½”ë” Self-Attention ê°€ì¤‘ì¹˜")
            st.markdown("**íˆíŠ¸ë§µ í•´ì„**:")
            st.markdown("- **í–‰**: ì–´í…ì…˜ì„ ì£¼ëŠ” ë‹¨ì–´ (Query)")
            st.markdown("- **ì—´**: ì–´í…ì…˜ì„ ë°›ëŠ” ë‹¨ì–´ (Key)")
            st.markdown("- **ìƒ‰ìƒ**: ë°ì„ìˆ˜ë¡ ë†’ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜")
            st.markdown("- **ëŒ€ê°ì„ **: ìê¸° ìì‹ ì—ê²Œ ì£¼ëŠ” ì–´í…ì…˜ (ë³´í†µ ë†’ìŒ)")
            
        elif attention_type == "ë””ì½”ë” Masked Self-Attention":
            st.markdown("### â›” ë””ì½”ë” ë§ˆìŠ¤í¬ë“œ Self-Attention (íƒ€ê¹ƒâ†’íƒ€ê¹ƒ)")
            st.markdown("**ëª©ì **: ì˜ì–´ ë¬¸ì¥ì„ ìƒì„±í•  ë•Œ, í˜„ì¬ ë‹¨ì–´ê°€ ì´ì „ì— ìƒì„±ëœ ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•˜ë„ë¡ í•˜ì—¬ ì •ë‹µì„ ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ í•¨")
            st.markdown("**íŠ¹ì§•**: ë¯¸ë˜ ë‹¨ì–´ ì •ë³´ë¥¼ ì°¨ë‹¨ (ë‹¨ë°©í–¥)")

            # C-1. Query, Key, Value ìƒì„± (íƒ€ê¹ƒì—ì„œ)
            st.markdown("#### C-1. Query, Key, Value ìƒì„± (íƒ€ê¹ƒ ì„ë² ë”© ì‚¬ìš©)")
            Qd_self = linear_projection(tgt_E, Wq)
            Kd_self = linear_projection(tgt_E, Wk)
            Vd_self = linear_projection(tgt_E, Wv)
            
            if show_steps:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown("**Query (íƒ€ê¹ƒ)**")
                    st.write("shape:", Qd_self.shape)
                    st.dataframe(np_to_df(Qd_self, row_idx=tgt_tokens))
                with col2:
                    st.markdown("**Key (íƒ€ê¹ƒ)**")
                    st.write("shape:", Kd_self.shape)
                    st.dataframe(np_to_df(Kd_self, row_idx=tgt_tokens))
                with col3:
                    st.markdown("**Value (íƒ€ê¹ƒ)**")
                    st.write("shape:", Vd_self.shape)
                    st.dataframe(np_to_df(Vd_self, row_idx=tgt_tokens))
            
            # C-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
            st.markdown("#### C-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°")
            dk_self = Qd_self.shape[-1]
            Sd_self = Qd_self @ Kd_self.T / np.sqrt(dk_self)
            
            if show_steps:
                st.write("**Score Matrix (ë§ˆìŠ¤í‚¹ ì „)**")
                st.dataframe(np_to_df(Sd_self, row_idx=tgt_tokens, col_idx=tgt_tokens))

            # C-3. ë§ˆìŠ¤í‚¹ ì ìš©
            st.markdown("#### C-3. ë§ˆìŠ¤í‚¹(Masking) ì ìš©")
            st.markdown("í˜„ì¬ ë‹¨ì–´ê°€ ë¯¸ë˜ì˜ ë‹¨ì–´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì§€ ëª»í•˜ë„ë¡, ì–´í…ì…˜ ìŠ¤ì½”ì–´ì˜ ì¼ë¶€ë¥¼ ì•„ì£¼ ì‘ì€ ê°’(-ë¬´í•œëŒ€)ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.")
            
            mask = np.triu(np.ones_like(Sd_self), k=1).astype(bool)
            Sd_self_masked = np.where(mask, -1e9, Sd_self)

            if show_steps:
                st.write("**Score Matrix (ë§ˆìŠ¤í‚¹ í›„)**")
                st.markdown("ëŒ€ê°ì„  ìœ„ìª½(ë¯¸ë˜ ì‹œì )ì˜ ê°’ë“¤ì´ `-1e9`ë¡œ ë³€ê²½ëœ ê²ƒì„ í™•ì¸í•˜ì„¸ìš”.")
                st.dataframe(np_to_df(Sd_self_masked, row_idx=tgt_tokens, col_idx=tgt_tokens))
            
            # C-4. Softmax ì ìš© ë° ìµœì¢… ì¶œë ¥ ê³„ì‚°
            st.markdown("#### C-4. Softmax ë° ìµœì¢… ì¶œë ¥")
            st.markdown("ë§ˆìŠ¤í‚¹ëœ ìŠ¤ì½”ì–´ì— Softmaxë¥¼ ì ìš©í•˜ë©´, ë¯¸ë˜ ë‹¨ì–´ì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” 0ì´ ë©ë‹ˆë‹¤.")
            Wd_self = softmax(Sd_self_masked, axis=-1)
            Cd_self = Wd_self @ Vd_self
            
            if show_steps:
                st.write("**Masked Attention Weights**")
                st.dataframe(np_to_df(Wd_self, row_idx=tgt_tokens, col_idx=tgt_tokens))
            
            # C-5. ì‹œê°í™”
            st.markdown("#### C-5. ë§ˆìŠ¤í¬ë“œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
            plot_heatmap(Wd_self, xticks=tgt_tokens, yticks=tgt_tokens, title="ë””ì½”ë” ë§ˆìŠ¤í¬ë“œ Self-Attention")
            st.markdown("**íˆíŠ¸ë§µ í•´ì„**: ê° í–‰(ë‹¨ì–´)ì€ ìê¸° ìì‹ ê³¼ ê·¸ ì´ì „ ë‹¨ì–´ë“¤ì—ê²Œë§Œ ì–´í…ì…˜ì„ ì£¼ê³ , ë¯¸ë˜ ë‹¨ì–´(ì˜¤ë¥¸ìª½)ëŠ” ì „í˜€ ì°¸ê³ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ê²€ì€ìƒ‰).")
            
        elif attention_type == "í¬ë¡œìŠ¤ Attention":
            st.markdown("### ğŸ”— ì¸ì½”ë”â€“ë””ì½”ë” Cross-Attention (íƒ€ê¹ƒâ†’ì†ŒìŠ¤)")
            st.markdown("**ëª©ì **: ì˜ì–´ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œ ì–´ë–¤ í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ì°¸ê³ í• ì§€ ê²°ì •")
            st.markdown("**íŠ¹ì§•**: íƒ€ê¹ƒì—ì„œ ì†ŒìŠ¤ë¡œì˜ ì •ë³´ íë¦„")
            
            # B-1. Query ìƒì„± (íƒ€ê¹ƒì—ì„œ)
            st.markdown("#### B-1. Query ìƒì„±: íƒ€ê¹ƒ ë‹¨ì–´ì—ì„œ")
            st.markdown("ì˜ì–´ ë‹¨ì–´ë“¤ì„ Queryë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            
            Qd = linear_projection(tgt_E, Wq)
            
            if show_steps:
                st.write("**Query (íƒ€ê¹ƒ)**")
                st.write("shape:", Qd.shape)
                st.dataframe(np_to_df(Qd, row_idx=tgt_tokens))
                st.markdown("**í•´ì„**: ì˜ì–´ ë‹¨ì–´ë“¤ì´ 'ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ì§€'ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.")
            
            # B-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° (íƒ€ê¹ƒ Query vs ì†ŒìŠ¤ Key)
            st.markdown("#### B-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°: íƒ€ê¹ƒ Query vs ì†ŒìŠ¤ Key")
            st.markdown("ê° ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë‹¨ì–´ë“¤ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ ê³„ì‚°í•©ë‹ˆë‹¤.")
            
            Ks = linear_projection(src_E, Wk)
            Vs = linear_projection(src_E, Wv)
            dk = Qd.shape[-1]
            Sd = Qd @ Ks.T / np.sqrt(dk)
            
            if show_steps:
                st.write("**Score Matrix (íƒ€ê¹ƒ vs ì†ŒìŠ¤)**")
                st.write("shape:", Sd.shape)
                st.dataframe(np_to_df(Sd, row_idx=tgt_tokens, col_idx=src_tokens))
                st.markdown("**í•´ì„**: ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ì–¼ë§ˆë‚˜ ì°¸ê³ í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.")
            
            # B-3. Softmax ì ìš©
            st.markdown("#### B-3. Softmax ì ìš©: í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜")
            st.markdown("ìŠ¤ì½”ì–´ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            
            Wd = softmax(Sd, axis=-1)
            
            if show_steps:
                st.write("**Softmax Weights**")
                st.write("shape:", Wd.shape)
                st.dataframe(np_to_df(Wd, row_idx=tgt_tokens, col_idx=src_tokens))
                st.markdown("**í•´ì„**: ê° ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë‹¨ì–´ë“¤ì— ì£¼ëŠ” ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤.")
            
            # B-4. ìµœì¢… ì¶œë ¥ ê³„ì‚° (ì†ŒìŠ¤ Value ì‚¬ìš©)
            st.markdown("#### B-4. ìµœì¢… ì¶œë ¥ ê³„ì‚°: ì†ŒìŠ¤ Value ì‚¬ìš©")
            st.markdown("ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¢…í•©í•©ë‹ˆë‹¤.")
            
            Cd = Wd @ Vs
            
            if show_steps:
                st.write("**Context Vector (ìµœì¢… ì¶œë ¥)**")
                st.write("shape:", Cd.shape)
                st.dataframe(np_to_df(Cd, row_idx=tgt_tokens))
                st.markdown("**í•´ì„**: ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë¬¸ì¥ì˜ ì •ë³´ë¥¼ ì¢…í•©í•œ í‘œí˜„ì…ë‹ˆë‹¤.")
            
            # B-5. ì‹œê°í™”
            st.markdown("#### B-5. í¬ë¡œìŠ¤ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
            plot_heatmap(Wd, xticks=src_tokens, yticks=tgt_tokens, title="í¬ë¡œìŠ¤ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (íƒ€ê¹ƒ í–‰ / ì†ŒìŠ¤ ì—´)")
            st.markdown("**íˆíŠ¸ë§µ í•´ì„**:")
            st.markdown("- **í–‰**: ì˜ì–´ ë‹¨ì–´ (íƒ€ê¹ƒ)")
            st.markdown("- **ì—´**: í•œêµ­ì–´ ë‹¨ì–´ (ì†ŒìŠ¤)")
            st.markdown("- **ìƒ‰ìƒ**: ë°ì„ìˆ˜ë¡ í•´ë‹¹ ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ë” ì°¸ê³ ")
            st.markdown("- **ì˜ˆì‹œ**: 'I'ê°€ 'ë‚˜ëŠ”'ì„ ì°¸ê³ , 'ate'ê°€ 'ë¨¹ì—ˆì–´'ë¥¼ ì°¸ê³ ")
        


# --------------------------------------------------
# íƒ­ 3: ë©€í‹°í—¤ë“œ ì‹œê°í™”
# --------------------------------------------------
with tabs[2]:
    st.subheader("ğŸ§© Multi-Head Attention ì‹œê°í™”")
    st.markdown("""
    í•œ ë²ˆì˜ ì–´í…ì…˜ì´ í•œ ê°€ì§€ ì¢…ë¥˜ì˜ ê´€ê³„ë§Œ ë³¸ë‹¤ë©´, ì—¬ëŸ¬ ë²ˆì˜ ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ê³„(ì˜ˆ: ë¬¸ë²•ì  ê´€ê³„, ì˜ë¯¸ì  ê´€ê³„ ë“±)ë¥¼ ë™ì‹œì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ **ë©€í‹°í—¤ë“œ ì–´í…ì…˜**ì˜ í•µì‹¬ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤.
    
    **ê³¼ì •:**
    1. **ë¶„í• (Split)**: ê¸°ì¡´ì˜ Q, K, Vë¥¼ ì—¬ëŸ¬ ê°œ('í—¤ë“œ'ì˜ ìˆ˜ë§Œí¼)ì˜ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    2. **ë³‘ë ¬ ì–´í…ì…˜(Parallel Attention)**: ê° ì¡°ê°(í—¤ë“œ)ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ Scaled Dot-Product ì–´í…ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê° í—¤ë“œëŠ” ì„œë¡œ ë‹¤ë¥¸ ì–´í…ì…˜ íŒ¨í„´ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.
    3. **ê²°í•©(Concatenate)**: ê° í—¤ë“œì—ì„œ ë‚˜ì˜¨ ê²°ê³¼(Context Vector)ë“¤ì„ ë‹¤ì‹œ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
    4. **ìµœì¢… ë³€í™˜(Final Projection)**: í•©ì³ì§„ ë²¡í„°ë¥¼ ìµœì¢… ì¶œë ¥ ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì„ í˜• ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œí‚µë‹ˆë‹¤.
    """)
    st.latex(r"\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O \quad \text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)")
    
    if analyze:
        st.markdown("---")
        
        # í—¤ë“œ ìˆ˜ ì„ íƒ
        num_heads = st.slider("í—¤ë“œ ìˆ˜ ì„ íƒ", min_value=1, max_value=8, value=4, help="ë¶„ì„í•  í—¤ë“œì˜ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”")
        
        st.markdown(f"### ğŸ¯ ë©€í‹°í—¤ë“œ í¬ë¡œìŠ¤ ì–´í…ì…˜ ì˜ˆì‹œ (Heads = {num_heads})")
        
        head_dim = dim // num_heads
        
        # í¸ì˜ìƒ, ê¸°ì¡´ Q,K,Vë¥¼ ë¶„í• í•˜ì—¬ ì‚¬ìš©
        Qd = linear_projection(tgt_E, Wq)
        Ks = linear_projection(src_E, Wk)
        Vs = linear_projection(src_E, Wv)
        
        # 1. ë¶„í•  (reshape)
        # (len, dim) -> (len, num_heads, head_dim) -> (num_heads, len, head_dim)
        Qd_heads = Qd.reshape(len(tgt_tokens), num_heads, head_dim).transpose(1, 0, 2)
        Ks_heads = Ks.reshape(len(src_tokens), num_heads, head_dim).transpose(1, 0, 2)
        Vs_heads = Vs.reshape(len(src_tokens), num_heads, head_dim).transpose(1, 0, 2)
        
        st.write(f"**1. ë¶„í• **: Q, K, Vë¥¼ ê°ê° `{num_heads}`ê°œì˜ í—¤ë“œë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.")
        st.write(f" - ì›ë³¸ Q shape: `{Qd.shape}` -> í—¤ë“œë³„ Q shape: `{Qd_heads.shape}` (num_heads, len, head_dim)")
        st.write(f" - ê° í—¤ë“œì˜ ì°¨ì›: `{head_dim}` (ì›ë³¸ ì°¨ì› `{dim}` Ã· í—¤ë“œ ìˆ˜ `{num_heads}`)")
        
        # 2. ë³‘ë ¬ ì–´í…ì…˜ & ì‹œê°í™”
        st.write(f"**2. ë³‘ë ¬ ì–´í…ì…˜**: ê° í—¤ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ì–´í…ì…˜ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")
        
        # í—¤ë“œë³„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì €ì¥
        head_weights = []
        head_outputs = []
        
        # í—¤ë“œë³„ ê°œë³„ í”Œë¡¯ìœ¼ë¡œ í‘œì‹œ
        for i in range(num_heads):
            context, weights, _ = scaled_dot_product_attention(Qd_heads[i], Ks_heads[i], Vs_heads[i])
            head_outputs.append(context)
            head_weights.append(weights)
            
            # ê°œë³„ í”Œë¡¯
            fig, ax = plt.subplots(figsize=(6, 5))
            im = ax.imshow(weights, cmap='viridis')
            ax.set_xticks(np.arange(len(src_tokens)))
            ax.set_yticks(np.arange(len(tgt_tokens)))
            ax.set_xticklabels(src_tokens, rotation=45, ha="right", fontsize=10)
            ax.set_yticklabels(tgt_tokens, fontsize=10)
            ax.set_title(f"Head {i+1} Attention Weights", fontsize=12)
            ax.set_xlabel("Source Tokens (í•œêµ­ì–´)", fontsize=10)
            ax.set_ylabel("Target Tokens (ì˜ì–´)", fontsize=10)
            
            # ê°’ ì£¼ì„ (í—¤ë“œê°€ ë§ìœ¼ë©´ ìƒëµ)
            if num_heads <= 4:
                for y in range(len(tgt_tokens)):
                    for x in range(len(src_tokens)):
                        ax.text(x, y, f"{weights[y, x]:.2f}", ha="center", va="center", fontsize=8)
            
            # ì»¬ëŸ¬ë°” ì¶”ê°€
            plt.colorbar(im, ax=ax, shrink=0.8)
            st.pyplot(fig)
            
            # í—¤ë“œë³„ í†µê³„ ì •ë³´
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric(f"Head {i+1} í‰ê·  ì–´í…ì…˜", f"{np.mean(weights):.4f}")
            with col2:
                st.metric(f"Head {i+1} ìµœëŒ€ ì–´í…ì…˜", f"{np.max(weights):.4f}")
            with col3:
                st.metric(f"Head {i+1} ì—”íŠ¸ë¡œí”¼", f"{calculate_entropy(weights):.4f}")
            
            st.markdown("---")

        # 3. í—¤ë“œ ê°„ ë¹„êµ ë¶„ì„
        st.markdown("### ğŸ” í—¤ë“œ ê°„ ë¹„êµ ë¶„ì„")
        
        # ëª¨ë“  í—¤ë“œì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ í•˜ë‚˜ì˜ íˆíŠ¸ë§µìœ¼ë¡œ ë¹„êµ
        if num_heads > 1:
            # í‰ê·  ì–´í…ì…˜ ê°€ì¤‘ì¹˜
            avg_weights = np.mean(head_weights, axis=0)
            
            fig, ax = plt.subplots(figsize=(8, 6))
            im = ax.imshow(avg_weights, cmap='viridis')
            ax.set_xticks(np.arange(len(src_tokens)))
            ax.set_yticks(np.arange(len(tgt_tokens)))
            ax.set_xticklabels(src_tokens, rotation=45, ha="right", fontsize=10)
            ax.set_yticklabels(tgt_tokens, fontsize=10)
            ax.set_title(f"í‰ê·  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ëª¨ë“  í—¤ë“œ)", fontsize=12)
            ax.set_xlabel("Source Tokens (í•œêµ­ì–´)", fontsize=10)
            ax.set_ylabel("Target Tokens (ì˜ì–´)", fontsize=10)
            
            # ê°’ ì£¼ì„
            for y in range(len(tgt_tokens)):
                for x in range(len(src_tokens)):
                    ax.text(x, y, f"{avg_weights[y, x]:.3f}", ha="center", va="center", fontsize=9)
            
            plt.colorbar(im, ax=ax, shrink=0.8)
            st.pyplot(fig)
            
            st.markdown("**í•´ì„**: ëª¨ë“  í—¤ë“œì˜ í‰ê· ì„ ì·¨í•˜ë©´, ê° í—¤ë“œê°€ í•™ìŠµí•œ ë‹¤ì–‘í•œ íŒ¨í„´ë“¤ì´ ì¢…í•©ëœ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # 4. ê²°í•© ë° ìµœì¢… ë³€í™˜
        st.write("**3. ê²°í•© ë° ìµœì¢… ë³€í™˜**: ê²°ê³¼ë“¤ì„ í•©ì¹˜ê³  ìµœì¢… ì¶œë ¥ ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.")
        concatenated = np.concatenate([h.transpose(1,0) for h in head_outputs], axis=-1).reshape(len(tgt_tokens), dim)
        # WoëŠ” ì‹œì—°ìš©ìœ¼ë¡œ ìƒëµ
        st.write(" - Concatenated shape:", concatenated.shape)
        st.dataframe(np_to_df(concatenated, row_idx=tgt_tokens))
        
        # 5. í—¤ë“œë³„ íŠ¹ì„± ë¶„ì„
        st.markdown("### ğŸ“Š í—¤ë“œë³„ íŠ¹ì„± ë¶„ì„")
        
        # í—¤ë“œë³„ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        entropies = [calculate_entropy(weights) for weights in head_weights]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(range(1, num_heads + 1), entropies)
        ax.set_xlabel("Head Number", fontsize=12)
        ax.set_ylabel("Entropy", fontsize=12)
        ax.set_title("ê° í—¤ë“œì˜ ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼", fontsize=14)
        ax.set_xticks(range(1, num_heads + 1))
        
        # ê°’ í‘œì‹œ
        for bar, entropy in zip(bars, entropies):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                   f'{entropy:.3f}', ha='center', va='bottom')
        
        st.pyplot(fig)
        
        st.markdown("**ì—”íŠ¸ë¡œí”¼ í•´ì„**:")
        st.markdown("- **ë†’ì€ ì—”íŠ¸ë¡œí”¼**: í•´ë‹¹ í—¤ë“œê°€ ì—¬ëŸ¬ ì†ŒìŠ¤ í† í°ì— ê³ ë¥´ê²Œ ì£¼ëª© (ë¶„ì‚°ëœ ì–´í…ì…˜)")
        st.markdown("- **ë‚®ì€ ì—”íŠ¸ë¡œí”¼**: í•´ë‹¹ í—¤ë“œê°€ íŠ¹ì • ì†ŒìŠ¤ í† í°ì— ì§‘ì¤‘ (ì§‘ì¤‘ëœ ì–´í…ì…˜)")
        
        # í—¤ë“œë³„ íŒ¨í„´ ìš”ì•½
        st.markdown("### ğŸ¯ í—¤ë“œë³„ íŒ¨í„´ ìš”ì•½")
        for i, weights in enumerate(head_weights):
            max_attention_idx = np.unravel_index(np.argmax(weights), weights.shape)
            max_tgt, max_src = max_attention_idx
            max_score = weights[max_attention_idx]
            
            st.markdown(f"**Head {i+1}**: '{tgt_tokens[max_tgt]}' â†’ '{src_tokens[max_src]}' (ì–´í…ì…˜: {max_score:.3f})")
        
        st.markdown("**í•´ì„**: ê° í—¤ë“œê°€ ì„œë¡œ ë‹¤ë¥¸ ì–´í…ì…˜ íŒ¨í„´ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ í—¤ë“œëŠ” ë‹¨ì–´ ëŒ€ ë‹¨ì–´ ê´€ê³„ì—, ë‹¤ë¥¸ í—¤ë“œëŠ” ì¢€ ë” ë¶„ì‚°ëœ ê´€ê³„ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.")

# --------------------------------------------------
# íƒ­ 4: ë§ˆìŠ¤í‚¹ & ì¸ê³¼ì„±
# --------------------------------------------------
with tabs[3]:
    st.subheader("â›” ë§ˆìŠ¤í‚¹ & ì¸ê³¼ì„± (Causality)")
    st.markdown("""
    ë””ì½”ë”ì—ì„œ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ë§ˆìŠ¤í‚¹ì˜ ì›ë¦¬ì™€ ìˆ˜í•™ì  í‘œí˜„ì„ ì‚´í´ë´…ë‹ˆë‹¤.
    """)
    
    if analyze:
        # ë§ˆìŠ¤í‚¹ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        seq_len = len(tgt_tokens)
        mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)
        
        st.markdown("### ğŸ”’ Causal Mask Matrix")
        st.markdown("**ìˆ˜ì‹**: `M_{ij} = 1[j â‰¤ i]` (j â‰¤ iì¼ ë•Œë§Œ 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0)")
        st.markdown("**ì˜ë¯¸**: ië²ˆì§¸ ìœ„ì¹˜ì—ì„œ jë²ˆì§¸ ìœ„ì¹˜ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€")
        
        # ë§ˆìŠ¤í‚¹ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™”
        fig, ax = plt.subplots(figsize=(8, 6))
        im = ax.imshow(mask.astype(int), cmap='RdYlBu_r')
        ax.set_xticks(np.arange(seq_len))
        ax.set_yticks(np.arange(seq_len))
        ax.set_xticklabels(tgt_tokens, rotation=45, ha="right")
        ax.set_yticklabels(tgt_tokens)
        ax.set_title("Causal Mask Matrix (1=ì°¸ê³  ê°€ëŠ¥, 0=ì°¸ê³  ë¶ˆê°€)", fontsize=12)
        
        # ê°’ ì£¼ì„
        for i in range(seq_len):
            for j in range(seq_len):
                text = ax.text(j, i, "âœ“" if mask[i, j] == False else "âœ—", 
                             ha="center", va="center", color="black", fontsize=14)
        
        st.pyplot(fig)
        
        st.markdown("**í•´ì„**:")
        st.markdown("- **âœ“ (í°ìƒ‰)**: ì°¸ê³  ê°€ëŠ¥í•œ ìœ„ì¹˜ (j â‰¤ i)")
        st.markdown("- **âœ— (íŒŒë€ìƒ‰)**: ì°¸ê³  ë¶ˆê°€ëŠ¥í•œ ìœ„ì¹˜ (j > i)")
        st.markdown("- **ëŒ€ê°ì„ **: ìê¸° ìì‹ ì€ í•­ìƒ ì°¸ê³  ê°€ëŠ¥")
        st.markdown("- **ì•„ë˜ìª½ ì‚¼ê°í˜•**: ì´ì „ í† í°ë“¤ì€ ì°¸ê³  ê°€ëŠ¥")
        st.markdown("- **ìœ„ìª½ ì‚¼ê°í˜•**: ë¯¸ë˜ í† í°ë“¤ì€ ì°¸ê³  ë¶ˆê°€ëŠ¥")
        
        # ë§ˆìŠ¤í‚¹ ì ìš© ì˜ˆì‹œ
        st.markdown("### ğŸ“Š ë§ˆìŠ¤í‚¹ ì ìš© ì˜ˆì‹œ")
        
        # ì˜ˆì‹œ ìŠ¤ì½”ì–´ ìƒì„±
        np.random.seed(42)
        example_scores = np.random.normal(0, 1, (seq_len, seq_len))
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ë§ˆìŠ¤í‚¹ ì „ ìŠ¤ì½”ì–´**")
            st.dataframe(np_to_df(example_scores, row_idx=tgt_tokens, col_idx=tgt_tokens))
        
        with col2:
            st.markdown("**ë§ˆìŠ¤í‚¹ í›„ ìŠ¤ì½”ì–´**")
            masked_scores = np.where(mask, -1e9, example_scores)
            st.dataframe(np_to_df(masked_scores, row_idx=tgt_tokens, col_idx=tgt_tokens))
            st.markdown("**ì°¸ê³ **: `-1e9`ëŠ” softmax í›„ 0ì´ ë˜ëŠ” ê°’")
        
        # Softmax ì ìš© ê²°ê³¼
        st.markdown("### ğŸ² Softmax ì ìš© ê²°ê³¼")
        
        # ë§ˆìŠ¤í‚¹ ì „í›„ softmax ë¹„êµ
        softmax_before = softmax(example_scores, axis=-1)
        softmax_after = softmax(masked_scores, axis=-1)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ë§ˆìŠ¤í‚¹ ì „ Softmax**")
            st.dataframe(np_to_df(softmax_before, row_idx=tgt_tokens, col_idx=tgt_tokens))
            st.markdown("**ë¬¸ì œ**: ë¯¸ë˜ í† í°ì— ëŒ€í•œ ì–´í…ì…˜ì´ 0ì´ ì•„ë‹˜")
        
        with col2:
            st.markdown("**ë§ˆìŠ¤í‚¹ í›„ Softmax**")
            st.dataframe(np_to_df(softmax_after, row_idx=tgt_tokens, col_idx=tgt_tokens))
            st.markdown("**í•´ê²°**: ë¯¸ë˜ í† í°ì— ëŒ€í•œ ì–´í…ì…˜ì´ 0ì´ ë¨")
        
        # ì¸ê³¼ì„±ì˜ ì¤‘ìš”ì„±
        st.markdown("### ğŸ¯ ì¸ê³¼ì„±ì˜ ì¤‘ìš”ì„±")
        st.markdown("""
        **í•™ìŠµ ì‹œ**: 
        - ì •ë‹µì„ ë¯¸ë¦¬ ë³´ë©´ ì•ˆ ë¨ (Cheating ë°©ì§€)
        - ì‹¤ì œ ì¶”ë¡  ìƒí™©ê³¼ ë™ì¼í•œ ì¡°ê±´ í•„ìš”
        
        **ì¶”ë¡  ì‹œ**: 
        - í•œ ë²ˆì— í•˜ë‚˜ì”© í† í° ìƒì„±
        - ì´ì „ì— ìƒì„±ëœ í† í°ë“¤ë§Œ ì°¸ê³  ê°€ëŠ¥
        
        **ì˜ˆì‹œ**: "I ate" ë‹¤ìŒì— "a"ë¥¼ ìƒì„±í•  ë•Œ
        - âœ… "I", "ate"ëŠ” ì°¸ê³  ê°€ëŠ¥
        - âŒ "meal"ì€ ì°¸ê³  ë¶ˆê°€ëŠ¥ (ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ)
        """)

# --------------------------------------------------
# íƒ­ 5: ì–´í…ì…˜ ì§€ë„
# --------------------------------------------------
with tabs[4]:
    st.subheader("ì–´í…ì…˜ ì§€ë„(Heatmap) ìƒì„¸ ë¶„ì„")
    if analyze:
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ ê³„ì‚°
        Qd = linear_projection(tgt_E, Wq)
        Ks = linear_projection(src_E, Wk)
        Vs = linear_projection(src_E, Wv)
        _, Wd, Sd = scaled_dot_product_attention(Qd, Ks, Vs)
        
        # 1. ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ íˆíŠ¸ë§µ
        st.markdown("### 1. ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤")
        plot_heatmap(Wd, xticks=src_tokens, yticks=tgt_tokens, title="Cross-Attention Matrix")
        st.markdown("**ì½ëŠ” ë²•**: í–‰=íƒ€ê¹ƒ(ì˜ì–´) í† í°, ì—´=ì†ŒìŠ¤(í•œêµ­ì–´) í† í°. ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ê°€ ê·¸ ì†ŒìŠ¤ ë‹¨ì–´ë¥¼ ë” ì°¸ê³ í•©ë‹ˆë‹¤.")
        
        # 2. ì›ë³¸ë‹¨ì–´ë³„ ì´ ì–´í…ì…˜ (ì—´ í•©ê³„)
        st.markdown("### 2. ì›ë³¸ë‹¨ì–´ë³„ ì´ ì–´í…ì…˜")
        src_total_attention = np.sum(Wd, axis=0)  # ê° ì†ŒìŠ¤ í† í°ì´ ë°›ëŠ” ì´ ì–´í…ì…˜
        fig, ax = plt.subplots(figsize=(10, 4))
        bars = ax.bar(src_tokens, src_total_attention)
        ax.set_title("ê° ì›ë³¸ ë‹¨ì–´ê°€ ë°›ëŠ” ì´ ì–´í…ì…˜", fontsize=12)
        ax.set_ylabel("ì´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜", fontsize=10)
        ax.set_xlabel("ì›ë³¸ ë‹¨ì–´", fontsize=10)
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, src_total_attention):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                   f'{value:.3f}', ha='center', va='bottom')
        plt.xticks(rotation=45)
        st.pyplot(fig)
        
        # 3. ë²ˆì—­ ë‹¨ì–´ë³„ ì–´í…ì…˜ ë¶„ì‚°
        st.markdown("### 3. ë²ˆì—­ ë‹¨ì–´ë³„ ì–´í…ì…˜ ë¶„ì‚°")
        tgt_attention_variance = np.var(Wd, axis=1)  # ê° íƒ€ê¹ƒ í† í°ì˜ ì–´í…ì…˜ ë¶„ì‚°
        fig, ax = plt.subplots(figsize=(10, 4))
        bars = ax.bar(tgt_tokens, tgt_attention_variance)
        ax.set_title("ê° ë²ˆì—­ ë‹¨ì–´ì˜ ì–´í…ì…˜ ë¶„ì‚°", fontsize=12)
        ax.set_ylabel("ë¶„ì‚°", fontsize=10)
        ax.set_xlabel("ë²ˆì—­ ë‹¨ì–´", fontsize=10)
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, tgt_attention_variance):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, 
                   f'{value:.3f}', ha='center', va='bottom')
        plt.xticks(rotation=45)
        st.pyplot(fig)
        
        # 4. ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        st.markdown("### 4. ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼")
        def calculate_entropy(attention_weights):
            """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
            # 0ì— ê°€ê¹Œìš´ ê°’ë“¤ì„ ì‘ì€ ì–‘ìˆ˜ë¡œ ëŒ€ì²´
            eps = 1e-10
            weights = np.maximum(attention_weights, eps)
            # ì •ê·œí™”
            weights = weights / np.sum(weights, axis=-1, keepdims=True)
            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: -sum(p * log(p))
            entropy = -np.sum(weights * np.log(weights), axis=-1)
            return entropy
        
        # ê° íƒ€ê¹ƒ í† í°ë³„ ì—”íŠ¸ë¡œí”¼
        tgt_entropy = calculate_entropy(Wd)
        fig, ax = plt.subplots(figsize=(10, 4))
        bars = ax.bar(tgt_tokens, tgt_entropy)
        ax.set_title("ê° ë²ˆì—­ ë‹¨ì–´ì˜ ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼", fontsize=12)
        ax.set_ylabel("ì—”íŠ¸ë¡œí”¼", fontsize=10)
        ax.set_xlabel("ë²ˆì—­ ë‹¨ì–´", fontsize=10)
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, tgt_entropy):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                   f'{value:.3f}', ha='center', va='bottom')
        plt.xticks(rotation=45)
        st.pyplot(fig)
        
        # 5. í†µê³„ ìš”ì•½
        st.markdown("### 5. ì–´í…ì…˜ í†µê³„ ìš”ì•½")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("í‰ê·  ì–´í…ì…˜", f"{np.mean(Wd):.4f}")
            st.metric("ìµœëŒ€ ì–´í…ì…˜", f"{np.max(Wd):.4f}")
            st.metric("ìµœì†Œ ì–´í…ì…˜", f"{np.min(Wd):.4f}")
        
        with col2:
            st.metric("ì–´í…ì…˜ í‘œì¤€í¸ì°¨", f"{np.std(Wd):.4f}")
            st.metric("ê°€ì¥ ì§‘ì¤‘ëœ íƒ€ê¹ƒ", tgt_tokens[np.argmin(tgt_entropy)])
            st.metric("ê°€ì¥ ë¶„ì‚°ëœ íƒ€ê¹ƒ", tgt_tokens[np.argmax(tgt_entropy)])
        
        with col3:
            st.metric("ê°€ì¥ ì£¼ëª©ë°›ëŠ” ì†ŒìŠ¤", src_tokens[np.argmax(src_total_attention)])
            st.metric("ê°€ì¥ ì£¼ëª©ë°›ëŠ” íƒ€ê¹ƒ", tgt_tokens[np.argmax(np.sum(Wd, axis=1))])
            st.metric("ì „ì²´ ì—”íŠ¸ë¡œí”¼", f"{np.mean(tgt_entropy):.4f}")
        
        # 6. ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„
        st.markdown("### 6. ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„")
        st.markdown("**ë†’ì€ ì—”íŠ¸ë¡œí”¼ (ë¶„ì‚°ëœ ì–´í…ì…˜)**: í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ê°€ ì—¬ëŸ¬ ì†ŒìŠ¤ ë‹¨ì–´ì— ê³ ë¥´ê²Œ ì£¼ëª©")
        st.markdown("**ë‚®ì€ ì—”íŠ¸ë¡œí”¼ (ì§‘ì¤‘ëœ ì–´í…ì…˜)**: í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ê°€ íŠ¹ì • ì†ŒìŠ¤ ë‹¨ì–´ì— ì§‘ì¤‘")
        st.markdown("**ë†’ì€ ë¶„ì‚°**: í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ì˜ ì–´í…ì…˜ì´ ë¶ˆê· ë“±í•˜ê²Œ ë¶„í¬")
        st.markdown("**ë‚®ì€ ë¶„ì‚°**: í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ì˜ ì–´í…ì…˜ì´ ê· ë“±í•˜ê²Œ ë¶„í¬")

# --------------------------------------------------
# íƒ­ 6: ì„ë² ë”© ë¶„ì„
# --------------------------------------------------
with tabs[5]:
    st.subheader("ì„ë² ë”© ìœ ì‚¬ë„ & ë¶„í¬")
    if analyze:
        all_tokens = ["[SRC]:"+t for t in src_tokens] + ["[TGT]:"+t for t in tgt_tokens]
        X = np.vstack([src_E, tgt_E])
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)
        cos = Xn @ Xn.T
        plot_heatmap(cos, xticks=all_tokens, yticks=all_tokens, title="ì½”ì‚¬ì¸ ìœ ì‚¬ë„")

        st.markdown("### PCA 2D ë¶„í¬(ì‹œì—°ìš©)")
        Z = (X - X.mean(0, keepdims=True))
        U, S, Vt = np.linalg.svd(Z, full_matrices=False)
        Z2 = Z @ Vt.T[:, :2]
        fig, ax = plt.subplots()
        ax.scatter(Z2[:len(src_tokens), 0], Z2[:len(src_tokens), 1], label="SRC")
        ax.scatter(Z2[len(src_tokens):, 0], Z2[len(src_tokens):, 1], label="TGT")
        for i, t in enumerate(all_tokens):
            ax.annotate(t, (Z2[i,0], Z2[i,1]), fontsize=8)
        ax.set_title("ì„ë² ë”© ë¶„í¬ (PCA 2D)", fontsize=12)
        ax.set_xlabel("ì²« ë²ˆì§¸ ì£¼ì„±ë¶„", fontsize=10)
        ax.set_ylabel("ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„", fontsize=10)
        ax.legend()
        st.pyplot(fig)

# --------------------------------------------------
# íƒ­ 7: PyTorch êµ¬í˜„
# --------------------------------------------------
with tabs[6]:
    st.subheader("PyTorchë¡œ ìŠ¤ì¼€ì¼ë“œë‹·í”„ë¡œë•íŠ¸ & ë©€í‹°í—¤ë“œ")
    
    if analyze:
        # PyTorch í…ì„œë¡œ ë³€í™˜
        src_E_torch = torch.tensor(src_E, dtype=torch.float32)
        tgt_E_torch = torch.tensor(tgt_E, dtype=torch.float32)
        Wq_torch = torch.tensor(Wq, dtype=torch.float32)
        Wk_torch = torch.tensor(Wk, dtype=torch.float32)
        Wv_torch = torch.tensor(Wv, dtype=torch.float32)
        
        # PyTorchë¡œ ì–´í…ì…˜ ê³„ì‚°
        Q_torch = torch.matmul(src_E_torch, Wq_torch)
        K_torch = torch.matmul(src_E_torch, Wk_torch)
        V_torch = torch.matmul(src_E_torch, Wv_torch)
        
        # Scaled Dot-Product Attention
        d_k = Q_torch.size(-1)
        scores_torch = torch.matmul(Q_torch, K_torch.transpose(-2, -1)) / math.sqrt(d_k)
        weights_torch = torch.softmax(scores_torch, dim=-1)
        context_torch = torch.matmul(weights_torch, V_torch)
        
        st.write("PyTorch ì–´í…ì…˜ ê²°ê³¼ shape:", context_torch.shape)
        st.dataframe(np_to_df(context_torch.detach().numpy(), row_idx=src_tokens))

# --------------------------------------------------
# íƒ­ 8: AI ì±—ë´‡ (ê°„ë‹¨ ì˜ˆì‹œ)
# --------------------------------------------------
with tabs[7]:
    st.subheader("ğŸ”¬ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

    if analyze:
        # ì–´í…ì…˜ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
        Qd = linear_projection(tgt_E, Wq)
        Ks = linear_projection(src_E, Wk)
        Vs = linear_projection(src_E, Wv)
        _, Wd, _ = scaled_dot_product_attention(Qd, Ks, Vs)
        
        st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì§ˆë¬¸")
        
        if len(tgt_tokens) > 0 and len(src_tokens) > 0:
            q1 = f"'{tgt_tokens[0]}' ë‹¨ì–´ê°€ ê°€ì¥ ì£¼ëª©í•œ ì›ë³¸ ë‹¨ì–´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            if st.button(q1):
                focused_idx = np.argmax(Wd[0])
                focused_word = src_tokens[focused_idx]
                st.write(f"**ë‹µë³€:** '{focused_word}' ë‹¨ì–´ì…ë‹ˆë‹¤. (ì–´í…ì…˜ ìŠ¤ì½”ì–´: {Wd[0, focused_idx]:.3f})")

            if len(src_tokens) > 1:
                q2 = f"'{src_tokens[1]}' ë‹¨ì–´ëŠ” ì–´ë–¤ ë²ˆì—­ ë‹¨ì–´ë¡œë¶€í„° ê°€ì¥ ë§ì€ ì£¼ëª©ì„ ë°›ì•˜ë‚˜ìš”?"
                if st.button(q2):
                    attending_idx = np.argmax(Wd[:, 1])
                    attending_word = tgt_tokens[attending_idx]
                    st.write(f"**ë‹µë³€:** '{attending_word}' ë‹¨ì–´ì…ë‹ˆë‹¤. (ì–´í…ì…˜ ìŠ¤ì½”ì–´: {Wd[attending_idx, 1]:.3f})")
            
            # ì¶”ê°€ ì§ˆë¬¸ë“¤
            q3 = "ê°€ì¥ ë†’ì€ ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ê°€ì§„ ë‹¨ì–´ ìŒì€ ë¬´ì—‡ì¸ê°€ìš”?"
            if st.button(q3):
                max_idx = np.unravel_index(np.argmax(Wd), Wd.shape)
                max_score = Wd[max_idx]
                tgt_word = tgt_tokens[max_idx[0]]
                src_word = src_tokens[max_idx[1]]
                st.write(f"**ë‹µë³€:** '{tgt_word}' â†’ '{src_word}' (ì–´í…ì…˜ ìŠ¤ì½”ì–´: {max_score:.3f})")
            
            q4 = "ì–´ë–¤ ì˜ì–´ ë‹¨ì–´ê°€ ê°€ì¥ ë¶„ì‚°ëœ ì–´í…ì…˜ì„ ë³´ì´ë‚˜ìš”?"
            if st.button(q4):
                # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
                def calculate_entropy(attention_weights):
                    eps = 1e-10
                    weights = np.maximum(attention_weights, eps)
                    weights = weights / np.sum(weights, axis=-1, keepdims=True)
                    entropy = -np.sum(weights * np.log(weights), axis=-1)
                    return entropy
                
                entropies = calculate_entropy(Wd)
                most_distributed_idx = np.argmax(entropies)
                most_distributed_word = tgt_tokens[most_distributed_idx]
                st.write(f"**ë‹µë³€:** '{most_distributed_word}' ë‹¨ì–´ì…ë‹ˆë‹¤. (ì—”íŠ¸ë¡œí”¼: {entropies[most_distributed_idx]:.3f})")
        
        st.markdown("---")
        st.markdown("### ğŸ’¬ ììœ  ì§ˆë¬¸")
        
        # ì‚¬ìš©ìê°€ ì§ì ‘ ì§ˆë¬¸í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë„ ìœ ì§€
        user_input = st.text_input("ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš” (ì˜ˆ: ì–´í…ì…˜ì´ë€?):", "")
        if st.button("ì§ˆë¬¸í•˜ê¸°"):
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ëª¨ë¸ ì‚¬ìš©)
            responses = {
                "ì–´í…ì…˜": "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ íŠ¹ì • ë¶€ë¶„ì— ì§‘ì¤‘í•˜ì—¬ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
                "transformer": "TransformerëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.",
                "self-attention": "Self-attentionì€ ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ì˜ ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ì„ ì°¸ì¡°í•˜ëŠ” ì–´í…ì…˜ì…ë‹ˆë‹¤.",
                "cross-attention": "Cross-attentionì€ ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê°„ì˜ ì–´í…ì…˜ì…ë‹ˆë‹¤.",
                "ë©€í‹°í—¤ë“œ": "ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì€ ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ë™ì‹œì— íŒŒì•…í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
                "ë§ˆìŠ¤í‚¹": "ë§ˆìŠ¤í‚¹ì€ ë””ì½”ë”ì—ì„œ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•˜ì—¬ í•™ìŠµ ì‹œ ì •ë‹µì„ ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.",
                "q": "Query(ì§ˆë¬¸)ëŠ” í˜„ì¬ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë˜ì§€ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.",
                "k": "Key(í‚¤)ëŠ” ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì´ 'ë‚˜ ì´ëŸ° ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì–´!'ë¼ê³  ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.",
                "v": "Value(ê°’)ëŠ” ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ, Queryì™€ ê°€ì¥ ì˜ ë§ëŠ” Keyì— ì—°ê²°ëœ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
            }
            
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ì–´í…ì…˜', 'transformer', 'self-attention', 'cross-attention', 'ë©€í‹°í—¤ë“œ', 'ë§ˆìŠ¤í‚¹', 'q', 'k', 'v' ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
            for keyword, resp in responses.items():
                if keyword.lower() in user_input.lower():
                    response = resp
                    break
            
            st.write("**AI ì‘ë‹µ:**", response)
            st.info("ğŸ’¡ ì´ëŠ” êµìœ¡ìš© ë°ëª¨ì…ë‹ˆë‹¤. ì‹¤ì œ AI ì±—ë´‡ì€ ë” ì •êµí•œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    else:
        st.warning("ë¨¼ì € 'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# --------------------------------------------------
# íƒ­ 9: í€´ì¦ˆ
# --------------------------------------------------
with tabs[8]:
    st.subheader("ğŸ“ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ í€´ì¦ˆ")
    st.markdown("í•™ìŠµí•œ ë‚´ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”!")
    
    # í€´ì¦ˆ 1: Q, K, Vì˜ ì—­í• 
    st.markdown("### ğŸ¯ í€´ì¦ˆ 1: Q, K, Vì˜ ì—­í• ")
    st.markdown("**ì§ˆë¬¸**: ì–´í…ì…˜ì—ì„œ Query(Q)ì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?")
    
    q1_answer = st.radio(
        "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”:",
        [
            "ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì´ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ëƒ„",
            "í˜„ì¬ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë˜ì§€ëŠ” ì§ˆë¬¸",
            "ê° ë‹¨ì–´ê°€ ì‹¤ì œë¡œ ë‹´ê³  ìˆëŠ” ì˜ë¯¸ ì •ë³´",
            "ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ìŠ¤ì¼€ì¼ë§ íŒ©í„°"
        ],
        key="q1"
    )
    
    if st.button("ì •ë‹µ í™•ì¸", key="check1"):
        if q1_answer == "í˜„ì¬ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë˜ì§€ëŠ” ì§ˆë¬¸":
            st.success("ğŸ‰ ì •ë‹µì…ë‹ˆë‹¤! QueryëŠ” 'ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?'ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.")
        else:
            st.error("âŒ í‹€ë ¸ìŠµë‹ˆë‹¤. QueryëŠ” í˜„ì¬ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë˜ì§€ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # í€´ì¦ˆ 2: ë§ˆìŠ¤í‚¹ì˜ ëª©ì 
    st.markdown("### ğŸ¯ í€´ì¦ˆ 2: ë§ˆìŠ¤í‚¹ì˜ ëª©ì ")
    st.markdown("**ì§ˆë¬¸**: ë””ì½”ë”ì—ì„œ ë§ˆìŠ¤í‚¹ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
    
    q2_answer = st.radio(
        "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”:",
        [
            "ê³„ì‚° ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´",
            "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´",
            "ë¯¸ë˜ í† í° ì •ë³´ë¥¼ ì°¸ê³ í•˜ì§€ ëª»í•˜ê²Œ í•˜ì—¬ ì •ë‹µì„ ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ í•˜ê¸° ìœ„í•´",
            "ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì˜ ë¶„ì‚°ì„ ì¤„ì´ê¸° ìœ„í•´"
        ],
        key="q2"
    )
    
    if st.button("ì •ë‹µ í™•ì¸", key="check2"):
        if q2_answer == "ë¯¸ë˜ í† í° ì •ë³´ë¥¼ ì°¸ê³ í•˜ì§€ ëª»í•˜ê²Œ í•˜ì—¬ ì •ë‹µì„ ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ í•˜ê¸° ìœ„í•´":
            st.success("ğŸ‰ ì •ë‹µì…ë‹ˆë‹¤! ë§ˆìŠ¤í‚¹ì€ í•™ìŠµ ì‹œ ì •ë‹µì„ ë¯¸ë¦¬ ë³´ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.")
        else:
            st.error("âŒ í‹€ë ¸ìŠµë‹ˆë‹¤. ë§ˆìŠ¤í‚¹ì€ ë¯¸ë˜ í† í° ì •ë³´ë¥¼ ì°¨ë‹¨í•˜ì—¬ ì •ë‹µì„ ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ í•©ë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # í€´ì¦ˆ 3: ë©€í‹°í—¤ë“œ ì–´í…ì…˜
    st.markdown("### ğŸ¯ í€´ì¦ˆ 3: ë©€í‹°í—¤ë“œ ì–´í…ì…˜")
    st.markdown("**ì§ˆë¬¸**: ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
    
    q3_answer = st.radio(
        "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”:",
        [
            "ê³„ì‚° ë³µì¡ë„ë¥¼ ì¤„ì„",
            "ì—¬ëŸ¬ ê°œì˜ 'í—¤ë“œ'ë¥¼ ë‘ì–´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•  ìˆ˜ ìˆìŒ",
            "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì„",
            "í•™ìŠµë¥ ì„ ìë™ìœ¼ë¡œ ì¡°ì •í•¨"
        ],
        key="q3"
    )
    
    if st.button("ì •ë‹µ í™•ì¸", key="check3"):
        if q3_answer == "ì—¬ëŸ¬ ê°œì˜ 'í—¤ë“œ'ë¥¼ ë‘ì–´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•  ìˆ˜ ìˆìŒ":
            st.success("ğŸ‰ ì •ë‹µì…ë‹ˆë‹¤! ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì€ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.error("âŒ í‹€ë ¸ìŠµë‹ˆë‹¤. ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì˜ í•µì‹¬ì€ ì—¬ëŸ¬ í—¤ë“œê°€ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.")
    
    # ì ìˆ˜ ê³„ì‚°
    st.markdown("---")
    st.markdown("### ğŸ“Š í€´ì¦ˆ ê²°ê³¼")
    
    # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•˜ê²Œ êµ¬í˜„ ê°€ëŠ¥)
    correct_answers = 0
    if 'q1' in st.session_state and st.session_state.q1 == "í˜„ì¬ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë˜ì§€ëŠ” ì§ˆë¬¸":
        correct_answers += 1
    if 'q2' in st.session_state and st.session_state.q2 == "ë¯¸ë˜ í† í° ì •ë³´ë¥¼ ì°¸ê³ í•˜ì§€ ëª»í•˜ê²Œ í•˜ì—¬ ì •ë‹µì„ ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ í•˜ê¸° ìœ„í•´":
        correct_answers += 1
    if 'q3' in st.session_state and st.session_state.q3 == "ì—¬ëŸ¬ ê°œì˜ 'í—¤ë“œ'ë¥¼ ë‘ì–´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•  ìˆ˜ ìˆìŒ":
        correct_answers += 1
    
    st.metric("ì •ë‹µ ìˆ˜", f"{correct_answers}/3")
    
    if correct_answers == 3:
        st.success("ğŸ‰ ì™„ë²½í•©ë‹ˆë‹¤! ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì˜ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
    elif correct_answers >= 2:
        st.info("ğŸ‘ ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤! ì¡°ê¸ˆë§Œ ë” í•™ìŠµí•˜ë©´ ë©ë‹ˆë‹¤.")
    else:
        st.warning("ğŸ“š ë” ë§ì€ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤. ìœ„ì˜ í•™ìŠµ ê°€ì´ë“œë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ì„¸ìš”.")

# --------------------------------------------------
# íƒ­ 10: ìš©ì–´ì‚¬ì „
# --------------------------------------------------
with tabs[9]:
    st.subheader("ğŸ“š ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ìš©ì–´ì‚¬ì „")
    st.markdown("í•µì‹¬ ê°œë…ë“¤ì„ ì •ë¦¬í•œ ìš©ì–´ì‚¬ì „ì…ë‹ˆë‹¤.")
    
    # ê²€ìƒ‰ ê¸°ëŠ¥
    search_term = st.text_input("ğŸ” ìš©ì–´ ê²€ìƒ‰:", placeholder="ì˜ˆ: attention, masking, transformer...")
    
    # ìš©ì–´ì‚¬ì „ ë°ì´í„°
    glossary = {
        "attention": {
            "í•œê¸€": "ì–´í…ì…˜",
            "ì •ì˜": "ì…ë ¥ì˜ íŠ¹ì • ë¶€ë¶„ì— ì§‘ì¤‘í•˜ì—¬ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜",
            "ì„¤ëª…": "ëª¨ë“  ì…ë ¥ì„ ë™ì‹œì— ê³ ë ¤í•˜ë˜, ì¤‘ìš”í•œ ë¶€ë¶„ì— ë” ì§‘ì¤‘í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤."
        },
        "query": {
            "í•œê¸€": "ì¿¼ë¦¬",
            "ì •ì˜": "í˜„ì¬ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë˜ì§€ëŠ” ì§ˆë¬¸",
            "ì„¤ëª…": "ë„ì„œê´€ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ ê°œë…ì…ë‹ˆë‹¤."
        },
        "key": {
            "í•œê¸€": "í‚¤",
            "ì •ì˜": "ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì´ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ì˜ íŠ¹ì§•",
            "ì„¤ëª…": "ë„ì„œê´€ì˜ ì±… ì œëª©ì´ë‚˜ í‚¤ì›Œë“œì™€ ê°™ì€ ì—­í• ì„ í•©ë‹ˆë‹¤."
        },
        "value": {
            "í•œê¸€": "ê°’",
            "ì •ì˜": "ê° ë‹¨ì–´ê°€ ì‹¤ì œë¡œ ë‹´ê³  ìˆëŠ” ì˜ë¯¸ ì •ë³´",
            "ì„¤ëª…": "ë„ì„œê´€ì˜ ì±… ë‚´ìš©ê³¼ ê°™ì€ ì‹¤ì œ ì •ë³´ì…ë‹ˆë‹¤."
        },
        "self-attention": {
            "í•œê¸€": "ì…€í”„ ì–´í…ì…˜",
            "ì •ì˜": "ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ì˜ ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ì„ ì°¸ì¡°í•˜ëŠ” ì–´í…ì…˜",
            "ì„¤ëª…": "ë¬¸ì¥ ë‚´ì—ì„œ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤."
        },
        "cross-attention": {
            "í•œê¸€": "í¬ë¡œìŠ¤ ì–´í…ì…˜",
            "ì •ì˜": "ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê°„ì˜ ì–´í…ì…˜",
            "ì„¤ëª…": "ë²ˆì—­ì—ì„œ ì†ŒìŠ¤ ì–¸ì–´ì™€ íƒ€ê¹ƒ ì–¸ì–´ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤."
        },
        "masking": {
            "í•œê¸€": "ë§ˆìŠ¤í‚¹",
            "ì •ì˜": "íŠ¹ì • ìœ„ì¹˜ì˜ ì •ë³´ë¥¼ ì°¨ë‹¨í•˜ëŠ” ê¸°ë²•",
            "ì„¤ëª…": "ë””ì½”ë”ì—ì„œ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•˜ì—¬ ì¸ê³¼ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤."
        },
        "multi-head": {
            "í•œê¸€": "ë©€í‹°í—¤ë“œ",
            "ì •ì˜": "ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ëŠ” êµ¬ì¡°",
            "ì„¤ëª…": "ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        "transformer": {
            "í•œê¸€": "íŠ¸ëœìŠ¤í¬ë¨¸",
            "ì •ì˜": "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜",
            "ì„¤ëª…": "RNNì˜ ìˆœì°¨ì  ì²˜ë¦¬ í•œê³„ë¥¼ ê·¹ë³µí•œ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ êµ¬ì¡°ì…ë‹ˆë‹¤."
        },
        "positional encoding": {
            "í•œê¸€": "ìœ„ì¹˜ ì¸ì½”ë”©",
            "ì •ì˜": "í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì„ë² ë”©ì— ì¶”ê°€í•˜ëŠ” ê¸°ë²•",
            "ì„¤ëª…": "ì–´í…ì…˜ì€ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ë³„ë„ë¡œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."
        }
    }
    
    # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
    if search_term:
        search_term = search_term.lower()
        found_terms = {k: v for k, v in glossary.items() if search_term in k or search_term in v["í•œê¸€"]}
        
        if found_terms:
            st.markdown(f"**ê²€ìƒ‰ ê²°ê³¼: '{search_term}'**")
            for term, info in found_terms.items():
                with st.expander(f"**{term}** ({info['í•œê¸€']})"):
                    st.markdown(f"**ì •ì˜**: {info['ì •ì˜']}")
                    st.markdown(f"**ì„¤ëª…**: {info['ì„¤ëª…']}")
        else:
            st.warning(f"'{search_term}'ì— ëŒ€í•œ ìš©ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì „ì²´ ìš©ì–´ì‚¬ì „ í‘œì‹œ
    st.markdown("### ğŸ“– ì „ì²´ ìš©ì–´ì‚¬ì „")
    for term, info in glossary.items():
        with st.expander(f"**{term}** ({info['í•œê¸€']})"):
            st.markdown(f"**ì •ì˜**: {info['ì •ì˜']}")
            st.markdown(f"**ì„¤ëª…**: {info['ì„¤ëª…']}")
    
    st.markdown("---")
    st.markdown("ğŸ’¡ **íŒ**: ìœ„ì˜ í•™ìŠµ ê°€ì´ë“œì™€ í•¨ê»˜ ìš©ì–´ì‚¬ì „ì„ ì°¸ê³ í•˜ë©´ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")