
# -*- coding: utf-8 -*-
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
from dataclasses import dataclass
import math

# í•œê¸€ í°íŠ¸ ì„¤ì •
import matplotlib.font_manager as fm
import platform

# ìš´ì˜ì²´ì œë³„ í•œê¸€ í°íŠ¸ ì„¤ì •
if platform.system() == 'Darwin':  # macOS
    plt.rcParams['font.family'] = 'AppleGothic'
elif platform.system() == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
else:  # Linux
    plt.rcParams['font.family'] = 'DejaVu Sans'
    
# ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False

st.set_page_config(page_title="Attention êµìœ¡ìš© ë°ëª¨", layout="wide")

# -----------------------------
# Utilities
# -----------------------------
def tokenize(text: str):
    """ê°„ë‹¨ í† í¬ë‚˜ì´ì €: ê³µë°±/êµ¬ë‘ì  ê¸°ì¤€ ë¶„ë¦¬"""
    import re
    # ë‹¨ì–´, ìˆ«ì, ì•„í¬ìŠ¤íŠ¸ë¡œí”¼/í•˜ì´í”ˆì„ í¬í•¨í•˜ëŠ” í† í° + ê·¸ ì™¸ êµ¬ë‘ì 
    tokens = re.findall(r"[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿0-9ê°€-í£]+(?:['-][A-Za-z0-9ê°€-í£]+)?|[^\sA-Za-z0-9ê°€-í£]", text.strip())
    return tokens if tokens else ["<empty>"]

def sinusoidal_positional_encoding(n: int, d: int):
    """ë…¼ë¬¸ì‹ ì‚¬ì¸-ì½”ì‚¬ì¸ ìœ„ì¹˜ì¸ì½”ë”©"""
    pe = np.zeros((n, d), dtype=np.float64)
    position = np.arange(n)[:, None]
    div_term = np.exp(np.arange(0, d, 2) * -(np.log(10000.0) / d))
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    return pe

def hash_vec(token: str, dim: int = 32) -> np.ndarray:
    """í† í°ë³„ ê²°ì •ì  í•´ì‹œ ì„ë² ë”©(ì‹œì—°ìš©)"""
    rs = np.random.RandomState(abs(hash(token)) % (2**32))
    return rs.normal(0, 1, size=dim)

def softmax(x, axis=-1):
    x = x - np.max(x, axis=axis, keepdims=True)
    ex = np.exp(x)
    return ex / np.sum(ex, axis=axis, keepdims=True)

def scaled_dot_product_attention(Q, K, V, mask=None):
    dk = Q.shape[-1]
    scores = Q @ K.T / np.sqrt(dk)  # [tgt, src]
    if mask is not None:
        scores = np.where(mask, scores, -1e9)
    weights = softmax(scores, axis=-1)
    context = weights @ V
    return context, weights, scores

def linear_projection(X, W, b=None):
    Y = X @ W
    if b is not None:
        Y += b
    return Y

def np_to_df(mat, row_idx=None, col_idx=None, floatfmt=6):
    import pandas as pd
    df = pd.DataFrame(np.round(mat.astype(float), floatfmt))
    if row_idx is not None:
        df.index = row_idx
    if col_idx is not None:
        df.columns = col_idx
    return df

def plot_heatmap(W, xticks, yticks, title=""):
    fig, ax = plt.subplots()
    im = ax.imshow(W)  # ìƒ‰ìƒì€ ê¸°ë³¸ê°’ ì‚¬ìš©(ê·œì •: íŠ¹ì • ìƒ‰ìƒ ì§€ì • ê¸ˆì§€)
    ax.set_xticks(np.arange(len(xticks)))
    ax.set_yticks(np.arange(len(yticks)))
    ax.set_xticklabels(xticks, rotation=45, ha="right")
    ax.set_yticklabels(yticks)
    ax.set_title(title, fontsize=12)
    # ê°’ ì£¼ì„(ì¹¸ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒëµ)
    if W.shape[0]*W.shape[1] <= 400:
        for i in range(W.shape[0]):
            for j in range(W.shape[1]):
                ax.text(j, i, f"{W[i, j]:.2f}", ha="center", va="center", fontsize=8)
    st.pyplot(fig)

def pca_2d(X, k=2):
    """numpy SVD ê¸°ë°˜ ê°„ë‹¨ PCA"""
    Xc = X - X.mean(axis=0, keepdims=True)
    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)
    Z = Xc @ Vt.T[:, :k]
    return Z

# -----------------------------
# Sidebar (ì…ë ¥/ì˜µì…˜/ë²„íŠ¼)
# -----------------------------
st.sidebar.header("ì…ë ¥ & ì˜µì…˜")
src_text = st.sidebar.text_area("ì›ë³¸ë¬¸ì¥ (í•œêµ­ì–´)", "ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆì–´")
tgt_text = st.sidebar.text_area("ë²ˆì—­ë¬¸ì¥ (ì˜ì–´)", "I ate a meal")
show_formula = st.sidebar.checkbox("ìˆ˜ì‹ í‘œì‹œ", value=True)
show_steps = st.sidebar.checkbox("ê³„ì‚° ê³¼ì •", value=True)
analyze = st.sidebar.button("ë¶„ì„ ì‹œì‘")

st.title("ğŸ¯ Attention êµìœ¡ìš© Streamlit ë°ëª¨")
st.caption("ë‹¨ê³„ë³„ ì–´í…ì…˜ Â· ë©€í‹°í—¤ë“œ ì–´í…ì…˜ Â· ì–´í…ì…˜ ì§€ë„ Â· ì„ë² ë”© ë¶„ì„ Â· PyTorch êµ¬í˜„ Â· AI ì±—ë´‡")

tabs = st.tabs(["ë‹¨ê³„ë³„ ì–´í…ì…˜", "ë©€í‹°í—¤ë“œ ì–´í…ì…˜", "ì–´í…ì…˜ ì§€ë„", "ì„ë² ë”© ë¶„ì„", "PyTorch êµ¬í˜„", "AI ì±—ë´‡"])

# ê³µí†µ ì „ì²˜ë¦¬
src_tokens = tokenize(src_text)
tgt_tokens = tokenize(tgt_text)

dim = 32  # ì„ë² ë”© ì°¨ì›(ì‹œì—°ìš©)
src_E = np.stack([hash_vec(t, dim) for t in src_tokens])
tgt_E = np.stack([hash_vec(t, dim) for t in tgt_tokens])
src_E = src_E + sinusoidal_positional_encoding(len(src_tokens), dim)
tgt_E = tgt_E + sinusoidal_positional_encoding(len(tgt_tokens), dim)

# ì„ í˜• ë³€í™˜ ê°€ì¤‘ì¹˜(ì‹œì—°ìš© ê³ ì • ë‚œìˆ˜)
np.random.seed(42)
Wq = np.random.normal(0, 1 / np.sqrt(dim), size=(dim, dim))
Wk = np.random.normal(0, 1 / np.sqrt(dim), size=(dim, dim))
Wv = np.random.normal(0, 1 / np.sqrt(dim), size=(dim, dim))

# --------------------------------------------------
# íƒ­ 1: ë‹¨ê³„ë³„ ì–´í…ì…˜
# --------------------------------------------------
with tabs[0]:
    st.subheader("1) Scaled Dot-Product Attention ë‹¨ê³„ë³„ ê³„ì‚°")
    
    # Q, K, Vì— ëŒ€í•œ ì§ê´€ì  ì„¤ëª… ì¶”ê°€
    with st.expander("ğŸ¤” Q, K, Vê°€ ë­”ê°€ìš”? (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)"):
        st.markdown("""
        ì–´í…ì…˜ì„ ë„ì„œê´€ì—ì„œ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ëŠ” ê³¼ì •ì— ë¹„ìœ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        
        - **Query (Q)**: **ë‚˜ì˜ ì§ˆë¬¸ í˜¹ì€ ê²€ìƒ‰ì–´**ì…ë‹ˆë‹¤. í˜„ì¬ ë‹¨ì–´ê°€ ë¬¸ì¥ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë˜ì§€ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.
        - **Key (K)**: ë„ì„œê´€ì— ìˆëŠ” **ì±…ë“¤ì˜ 'í•µì‹¬ í‚¤ì›Œë“œ'ë‚˜ 'ê¼¬ë¦¬í‘œ'** ì™€ ê°™ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ë“¤ì´ "ë‚˜ ì´ëŸ° ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì–´!"ë¼ê³  ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
        - **Value (V)**: ì±…ì˜ **ì‹¤ì œ 'ë‚´ìš©'** ì…ë‹ˆë‹¤. ë‚´ ì§ˆë¬¸(Query)ê³¼ ê°€ì¥ ê´€ë ¨ì´ ê¹Šì€ í‚¤ì›Œë“œ(Key)ë¥¼ ì°¾ì•˜ë‹¤ë©´, ê·¸ í‚¤ì›Œë“œê°€ ë¶™ì–´ìˆëŠ” ì±…ì˜ ì‹¤ì œ ë‚´ìš©(Value)ì„ ê°€ì ¸ì™€ ì°¸ê³ í•©ë‹ˆë‹¤.
        
        **ê²°ë¡ ì ìœ¼ë¡œ ì–´í…ì…˜ì€, ë‚˜ì˜ Queryì™€ ê°€ì¥ ì˜ ë§ëŠ” Keyë¥¼ ì°¾ì•„ì„œ, í•´ë‹¹ Keyì— ì—°ê²°ëœ Valueë¥¼ ê°€ì ¸ì˜¤ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.**
        """)
    
    if show_formula:
        st.markdown("**ê³µì‹**")
        st.latex(r"Q = XW_Q,\ K = XW_K,\ V = XW_V")
        st.latex(r"\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V")
        st.caption("â€» ì—¬ê¸°ì„œëŠ” êµìœ¡ìš©ìœ¼ë¡œ ì†ŒìŠ¤/íƒ€ê¹ƒ ì„ë² ë”©ì— ê°™ì€ ì°¨ì›/ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    st.markdown("**í† í°**")
    c1, c2 = st.columns(2)
    with c1:
        st.write("ì†ŒìŠ¤ í† í°:", src_tokens)
    with c2:
        st.write("íƒ€ê¹ƒ í† í°:", tgt_tokens)

    if analyze:
        # A. ì¸ì½”ë” Self-Attention (ì†ŒìŠ¤â†’ì†ŒìŠ¤)
        st.markdown("### A. ì¸ì½”ë” Self-Attention (ì†ŒìŠ¤â†’ì†ŒìŠ¤)")
        st.markdown("**ëª©ì **: í•œêµ­ì–´ ë¬¸ì¥ ë‚´ì—ì„œ ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ í•™ìŠµ")
        
        # A-1. ì„ í˜• ë³€í™˜ (Query, Key, Value ìƒì„±)
        st.markdown("#### A-1. ì„ í˜• ë³€í™˜: Query, Key, Value ìƒì„±")
        st.markdown("ê° ë‹¨ì–´ ì„ë² ë”©ì„ Query, Key, Valueë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
        
        Qs = linear_projection(src_E, Wq)
        Ks = linear_projection(src_E, Wk)
        Vs = linear_projection(src_E, Wv)
        
        if show_steps:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("**Query (Q)**")
                st.write("shape:", Qs.shape)
                st.dataframe(np_to_df(Qs, row_idx=src_tokens))
            with col2:
                st.markdown("**Key (K)**")
                st.write("shape:", Ks.shape)
                st.dataframe(np_to_df(Ks, row_idx=src_tokens))
            with col3:
                st.markdown("**Value (V)**")
                st.write("shape:", Vs.shape)
                st.dataframe(np_to_df(Vs, row_idx=src_tokens))
        
        # A-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
        st.markdown("#### A-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°: QKáµ€/âˆšd")
        st.markdown("ê° ë‹¨ì–´ ìŒ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.")
        
        dk = Qs.shape[-1]
        Ss = Qs @ Ks.T / np.sqrt(dk)
        
        if show_steps:
            st.write("**Score Matrix (QKáµ€/âˆšd)**")
            st.write("shape:", Ss.shape)
            st.dataframe(np_to_df(Ss, row_idx=src_tokens, col_idx=src_tokens))
            st.markdown("**í•´ì„**: ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ë‹¨ì–´ ìŒì´ ë” ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
        
        # A-3. Softmax ì ìš©
        st.markdown("#### A-3. Softmax ì ìš©: í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜")
        st.markdown("ìŠ¤ì½”ì–´ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•˜ì—¬ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.")
        
        Ws = softmax(Ss, axis=-1)
        
        if show_steps:
            st.write("**Softmax Weights**")
            st.write("shape:", Ws.shape)
            st.dataframe(np_to_df(Ws, row_idx=src_tokens, col_idx=src_tokens))
            st.markdown("**í•´ì„**: ê° í–‰ì˜ í•©ì´ 1ì´ ë˜ë©°, ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ë‹¨ì–´ì— ë” ì§‘ì¤‘í•©ë‹ˆë‹¤.")
        
        # A-4. ìµœì¢… ì¶œë ¥ ê³„ì‚°
        st.markdown("#### A-4. ìµœì¢… ì¶œë ¥ ê³„ì‚°: Weighted Sum")
        st.markdown("ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Valueì˜ ê°€ì¤‘ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")
        
        Cs = Ws @ Vs
        
        if show_steps:
            st.write("**Context Vector (ìµœì¢… ì¶œë ¥)**")
            st.write("shape:", Cs.shape)
            st.dataframe(np_to_df(Cs, row_idx=src_tokens))
            st.markdown("**í•´ì„**: ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ ì •ë³´ë¥¼ ì¢…í•©í•œ ìƒˆë¡œìš´ í‘œí˜„ì…ë‹ˆë‹¤.")
        
        # A-5. ì‹œê°í™”
        st.markdown("#### A-5. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
        plot_heatmap(Ws, xticks=src_tokens, yticks=src_tokens, title="ì¸ì½”ë” Self-Attention ê°€ì¤‘ì¹˜")
        st.markdown("**íˆíŠ¸ë§µ í•´ì„**:")
        st.markdown("- **í–‰**: ì–´í…ì…˜ì„ ì£¼ëŠ” ë‹¨ì–´ (Query)")
        st.markdown("- **ì—´**: ì–´í…ì…˜ì„ ë°›ëŠ” ë‹¨ì–´ (Key)")
        st.markdown("- **ìƒ‰ìƒ**: ë°ì„ìˆ˜ë¡ ë†’ì€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜")
        st.markdown("- **ëŒ€ê°ì„ **: ìê¸° ìì‹ ì—ê²Œ ì£¼ëŠ” ì–´í…ì…˜ (ë³´í†µ ë†’ìŒ)")

        # êµ¬ë¶„ì„ 
        st.markdown("---")
        
        # B. ì¸ì½”ë”â€“ë””ì½”ë” Cross-Attention (íƒ€ê¹ƒâ†’ì†ŒìŠ¤)
        st.markdown("### B. ì¸ì½”ë”â€“ë””ì½”ë” Cross-Attention (íƒ€ê¹ƒâ†’ì†ŒìŠ¤)")
        st.markdown("**ëª©ì **: ì˜ì–´ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œ ì–´ë–¤ í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ì°¸ê³ í• ì§€ ê²°ì •")
        
        # B-1. Query ìƒì„± (íƒ€ê¹ƒì—ì„œ)
        st.markdown("#### B-1. Query ìƒì„±: íƒ€ê¹ƒ ë‹¨ì–´ì—ì„œ")
        st.markdown("ì˜ì–´ ë‹¨ì–´ë“¤ì„ Queryë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
        
        Qd = linear_projection(tgt_E, Wq)
        
        if show_steps:
            st.write("**Query (íƒ€ê¹ƒ)**")
            st.write("shape:", Qd.shape)
            st.dataframe(np_to_df(Qd, row_idx=tgt_tokens))
            st.markdown("**í•´ì„**: ì˜ì–´ ë‹¨ì–´ë“¤ì´ 'ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ì§€'ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.")
        
        # B-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° (íƒ€ê¹ƒ Query vs ì†ŒìŠ¤ Key)
        st.markdown("#### B-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°: íƒ€ê¹ƒ Query vs ì†ŒìŠ¤ Key")
        st.markdown("ê° ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë‹¨ì–´ë“¤ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ ê³„ì‚°í•©ë‹ˆë‹¤.")
        
        Sd = Qd @ Ks.T / np.sqrt(dk)
        
        if show_steps:
            st.write("**Score Matrix (íƒ€ê¹ƒ vs ì†ŒìŠ¤)**")
            st.write("shape:", Sd.shape)
            st.dataframe(np_to_df(Sd, row_idx=tgt_tokens, col_idx=src_tokens))
            st.markdown("**í•´ì„**: ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ì–¼ë§ˆë‚˜ ì°¸ê³ í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.")
        
        # B-3. Softmax ì ìš©
        st.markdown("#### B-3. Softmax ì ìš©: í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜")
        st.markdown("ìŠ¤ì½”ì–´ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
        
        Wd = softmax(Sd, axis=-1)
        
        if show_steps:
            st.write("**Softmax Weights**")
            st.write("shape:", Wd.shape)
            st.dataframe(np_to_df(Wd, row_idx=tgt_tokens, col_idx=src_tokens))
            st.markdown("**í•´ì„**: ê° ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë‹¨ì–´ë“¤ì— ì£¼ëŠ” ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤.")
        
        # B-4. ìµœì¢… ì¶œë ¥ ê³„ì‚° (ì†ŒìŠ¤ Value ì‚¬ìš©)
        st.markdown("#### B-4. ìµœì¢… ì¶œë ¥ ê³„ì‚°: ì†ŒìŠ¤ Value ì‚¬ìš©")
        st.markdown("ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¢…í•©í•©ë‹ˆë‹¤.")
        
        Cd = Wd @ Vs
        
        if show_steps:
            st.write("**Context Vector (ìµœì¢… ì¶œë ¥)**")
            st.write("shape:", Cd.shape)
            st.dataframe(np_to_df(Cd, row_idx=tgt_tokens))
            st.markdown("**í•´ì„**: ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë¬¸ì¥ì˜ ì •ë³´ë¥¼ ì¢…í•©í•œ í‘œí˜„ì…ë‹ˆë‹¤.")
        
        # B-5. ì‹œê°í™”
        st.markdown("#### B-5. í¬ë¡œìŠ¤ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
        plot_heatmap(Wd, xticks=src_tokens, yticks=tgt_tokens, title="í¬ë¡œìŠ¤ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (íƒ€ê¹ƒ í–‰ / ì†ŒìŠ¤ ì—´)")
        st.markdown("**íˆíŠ¸ë§µ í•´ì„**:")
        st.markdown("- **í–‰**: ì˜ì–´ ë‹¨ì–´ (íƒ€ê¹ƒ)")
        st.markdown("- **ì—´**: í•œêµ­ì–´ ë‹¨ì–´ (ì†ŒìŠ¤)")
        st.markdown("- **ìƒ‰ìƒ**: ë°ì„ìˆ˜ë¡ í•´ë‹¹ ì˜ì–´ ë‹¨ì–´ê°€ í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ë” ì°¸ê³ ")
        st.markdown("- **ì˜ˆì‹œ**: 'I'ê°€ 'ë‚˜ëŠ”'ì„ ì°¸ê³ , 'ate'ê°€ 'ë¨¹ì—ˆì–´'ë¥¼ ì°¸ê³ ")

        # êµ¬ë¶„ì„ 
        st.markdown("---")
        
        # C. ë””ì½”ë” ë§ˆìŠ¤í¬ë“œ Self-Attention (íƒ€ê¹ƒâ†’íƒ€ê¹ƒ)
        st.markdown("### C. ë””ì½”ë” ë§ˆìŠ¤í¬ë“œ Self-Attention (íƒ€ê¹ƒâ†’íƒ€ê¹ƒ)")
        st.markdown("**ëª©ì **: ì˜ì–´ ë¬¸ì¥ì„ ìƒì„±í•  ë•Œ, í˜„ì¬ ë‹¨ì–´ê°€ ì´ì „ì— ìƒì„±ëœ ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•˜ë„ë¡ í•˜ì—¬ ì •ë‹µì„ ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ í•¨")

        # C-1. Query, Key, Value ìƒì„± (íƒ€ê¹ƒì—ì„œ)
        st.markdown("#### C-1. Query, Key, Value ìƒì„± (íƒ€ê¹ƒ ì„ë² ë”© ì‚¬ìš©)")
        Qd_self = linear_projection(tgt_E, Wq)
        Kd_self = linear_projection(tgt_E, Wk)
        Vd_self = linear_projection(tgt_E, Wv)
        
        if show_steps:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("**Query (íƒ€ê¹ƒ)**")
                st.write("shape:", Qd_self.shape)
                st.dataframe(np_to_df(Qd_self, row_idx=tgt_tokens))
            with col2:
                st.markdown("**Key (íƒ€ê¹ƒ)**")
                st.write("shape:", Kd_self.shape)
                st.dataframe(np_to_df(Kd_self, row_idx=tgt_tokens))
            with col3:
                st.markdown("**Value (íƒ€ê¹ƒ)**")
                st.write("shape:", Vd_self.shape)
                st.dataframe(np_to_df(Vd_self, row_idx=tgt_tokens))
        
        # C-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
        st.markdown("#### C-2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°")
        dk_self = Qd_self.shape[-1]
        Sd_self = Qd_self @ Kd_self.T / np.sqrt(dk_self)
        
        if show_steps:
            st.write("**Score Matrix (ë§ˆìŠ¤í‚¹ ì „)**")
            st.dataframe(np_to_df(Sd_self, row_idx=tgt_tokens, col_idx=tgt_tokens))

        # C-3. ë§ˆìŠ¤í‚¹ ì ìš©
        st.markdown("#### C-3. ë§ˆìŠ¤í‚¹(Masking) ì ìš©")
        st.markdown("í˜„ì¬ ë‹¨ì–´ê°€ ë¯¸ë˜ì˜ ë‹¨ì–´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì§€ ëª»í•˜ë„ë¡, ì–´í…ì…˜ ìŠ¤ì½”ì–´ì˜ ì¼ë¶€ë¥¼ ì•„ì£¼ ì‘ì€ ê°’(-ë¬´í•œëŒ€)ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.")
        
        mask = np.triu(np.ones_like(Sd_self), k=1).astype(bool)
        Sd_self_masked = np.where(mask, -1e9, Sd_self)

        if show_steps:
            st.write("**Score Matrix (ë§ˆìŠ¤í‚¹ í›„)**")
            st.markdown("ëŒ€ê°ì„  ìœ„ìª½(ë¯¸ë˜ ì‹œì )ì˜ ê°’ë“¤ì´ `-1e9`ë¡œ ë³€ê²½ëœ ê²ƒì„ í™•ì¸í•˜ì„¸ìš”.")
            st.dataframe(np_to_df(Sd_self_masked, row_idx=tgt_tokens, col_idx=tgt_tokens))
        
        # C-4. Softmax ì ìš© ë° ìµœì¢… ì¶œë ¥ ê³„ì‚°
        st.markdown("#### C-4. Softmax ë° ìµœì¢… ì¶œë ¥")
        st.markdown("ë§ˆìŠ¤í‚¹ëœ ìŠ¤ì½”ì–´ì— Softmaxë¥¼ ì ìš©í•˜ë©´, ë¯¸ë˜ ë‹¨ì–´ì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” 0ì´ ë©ë‹ˆë‹¤.")
        Wd_self = softmax(Sd_self_masked, axis=-1)
        Cd_self = Wd_self @ Vd_self
        
        if show_steps:
            st.write("**Masked Attention Weights**")
            st.dataframe(np_to_df(Wd_self, row_idx=tgt_tokens, col_idx=tgt_tokens))
        
        # C-5. ì‹œê°í™”
        st.markdown("#### C-5. ë§ˆìŠ¤í¬ë“œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”")
        plot_heatmap(Wd_self, xticks=tgt_tokens, yticks=tgt_tokens, title="ë””ì½”ë” ë§ˆìŠ¤í¬ë“œ Self-Attention")
        st.markdown("**íˆíŠ¸ë§µ í•´ì„**: ê° í–‰(ë‹¨ì–´)ì€ ìê¸° ìì‹ ê³¼ ê·¸ ì´ì „ ë‹¨ì–´ë“¤ì—ê²Œë§Œ ì–´í…ì…˜ì„ ì£¼ê³ , ë¯¸ë˜ ë‹¨ì–´(ì˜¤ë¥¸ìª½)ëŠ” ì „í˜€ ì°¸ê³ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ê²€ì€ìƒ‰).")

# --------------------------------------------------
# íƒ­ 2: ë©€í‹°í—¤ë“œ ì–´í…ì…˜
# --------------------------------------------------
with tabs[1]:
    st.subheader("ğŸ’¡ Multi-Head Attentionì˜ ì›ë¦¬")
    st.markdown("""
    í•œ ë²ˆì˜ ì–´í…ì…˜ì´ í•œ ê°€ì§€ ì¢…ë¥˜ì˜ ê´€ê³„ë§Œ ë³¸ë‹¤ë©´, ì—¬ëŸ¬ ë²ˆì˜ ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ê³„(ì˜ˆ: ë¬¸ë²•ì  ê´€ê³„, ì˜ë¯¸ì  ê´€ê³„ ë“±)ë¥¼ ë™ì‹œì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ **ë©€í‹°í—¤ë“œ ì–´í…ì…˜**ì˜ í•µì‹¬ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤.
    
    **ê³¼ì •:**
    1. **ë¶„í• (Split)**: ê¸°ì¡´ì˜ Q, K, Vë¥¼ ì—¬ëŸ¬ ê°œ('í—¤ë“œ'ì˜ ìˆ˜ë§Œí¼)ì˜ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    2. **ë³‘ë ¬ ì–´í…ì…˜(Parallel Attention)**: ê° ì¡°ê°(í—¤ë“œ)ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ Scaled Dot-Product ì–´í…ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê° í—¤ë“œëŠ” ì„œë¡œ ë‹¤ë¥¸ ì–´í…ì…˜ íŒ¨í„´ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.
    3. **ê²°í•©(Concatenate)**: ê° í—¤ë“œì—ì„œ ë‚˜ì˜¨ ê²°ê³¼(Context Vector)ë“¤ì„ ë‹¤ì‹œ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
    4. **ìµœì¢… ë³€í™˜(Final Projection)**: í•©ì³ì§„ ë²¡í„°ë¥¼ ìµœì¢… ì¶œë ¥ ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì„ í˜• ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œí‚µë‹ˆë‹¤.
    """)
    st.latex(r"\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O \quad \text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)")
    
    if analyze:
        st.markdown("---")
        st.markdown("### ë©€í‹°í—¤ë“œ í¬ë¡œìŠ¤ ì–´í…ì…˜ ì˜ˆì‹œ (Heads = 4)")
        
        num_heads = 4
        head_dim = dim // num_heads
        
        # í¸ì˜ìƒ, ê¸°ì¡´ Q,K,Vë¥¼ ë¶„í• í•˜ì—¬ ì‚¬ìš©
        Qd = linear_projection(tgt_E, Wq)
        Ks = linear_projection(src_E, Wk)
        Vs = linear_projection(src_E, Wv)
        
        # 1. ë¶„í•  (reshape)
        # (len, dim) -> (len, num_heads, head_dim) -> (num_heads, len, head_dim)
        Qd_heads = Qd.reshape(len(tgt_tokens), num_heads, head_dim).transpose(1, 0, 2)
        Ks_heads = Ks.reshape(len(src_tokens), num_heads, head_dim).transpose(1, 0, 2)
        Vs_heads = Vs.reshape(len(src_tokens), num_heads, head_dim).transpose(1, 0, 2)
        
        st.write(f"**1. ë¶„í• **: Q, K, Vë¥¼ ê°ê° `{num_heads}`ê°œì˜ í—¤ë“œë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.")
        st.write(f" - ì›ë³¸ Q shape: `{Qd.shape}` -> í—¤ë“œë³„ Q shape: `{Qd_heads.shape}` (num_heads, len, head_dim)")
        
        # 2. ë³‘ë ¬ ì–´í…ì…˜ & ì‹œê°í™”
        st.write(f"**2. ë³‘ë ¬ ì–´í…ì…˜**: ê° í—¤ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ì–´í…ì…˜ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")
        
        cols = st.columns(num_heads)
        head_outputs = []
        for i in range(num_heads):
            with cols[i]:
                context, weights, _ = scaled_dot_product_attention(Qd_heads[i], Ks_heads[i], Vs_heads[i])
                head_outputs.append(context)
                
                fig, ax = plt.subplots()
                im = ax.imshow(weights)
                ax.set_xticks(np.arange(len(src_tokens)))
                ax.set_yticks(np.arange(len(tgt_tokens)))
                ax.set_xticklabels(src_tokens, rotation=90, ha="right", fontsize=8)
                ax.set_yticklabels(tgt_tokens, fontsize=8)
                ax.set_title(f"Head {i+1}", fontsize=10)
                st.pyplot(fig)

        st.markdown("**í•´ì„**: ê° í—¤ë“œ(Head)ê°€ ì„œë¡œ ë‹¤ë¥¸ ì–´í…ì…˜ íŒ¨í„´ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ í—¤ë“œëŠ” ë‹¨ì–´ ëŒ€ ë‹¨ì–´ ê´€ê³„ì—, ë‹¤ë¥¸ í—¤ë“œëŠ” ì¢€ ë” ë¶„ì‚°ëœ ê´€ê³„ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.")
        
        # 3. ê²°í•© ë° ìµœì¢… ë³€í™˜
        st.write("**3. ê²°í•© ë° ìµœì¢… ë³€í™˜**: ê²°ê³¼ë“¤ì„ í•©ì¹˜ê³  ìµœì¢… ì¶œë ¥ ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.")
        concatenated = np.concatenate([h.transpose(1,0) for h in head_outputs], axis=-1).reshape(len(tgt_tokens), dim)
        # WoëŠ” ì‹œì—°ìš©ìœ¼ë¡œ ìƒëµ
        st.write(" - Concatenated shape:", concatenated.shape)
        st.dataframe(np_to_df(concatenated, row_idx=tgt_tokens))

# --------------------------------------------------
# íƒ­ 3: ì–´í…ì…˜ ì§€ë„
# --------------------------------------------------
with tabs[2]:
    st.subheader("ì–´í…ì…˜ ì§€ë„(Heatmap) ìƒì„¸ ë¶„ì„")
    if analyze:
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ ê³„ì‚°
        Qd = linear_projection(tgt_E, Wq)
        Ks = linear_projection(src_E, Wk)
        Vs = linear_projection(src_E, Wv)
        _, Wd, Sd = scaled_dot_product_attention(Qd, Ks, Vs)
        
        # 1. ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ íˆíŠ¸ë§µ
        st.markdown("### 1. ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤")
        plot_heatmap(Wd, xticks=src_tokens, yticks=tgt_tokens, title="Cross-Attention Matrix")
        st.markdown("**ì½ëŠ” ë²•**: í–‰=íƒ€ê¹ƒ(ì˜ì–´) í† í°, ì—´=ì†ŒìŠ¤(í•œêµ­ì–´) í† í°. ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ê°€ ê·¸ ì†ŒìŠ¤ ë‹¨ì–´ë¥¼ ë” ì°¸ê³ í•©ë‹ˆë‹¤.")
        
        # 2. ì›ë³¸ë‹¨ì–´ë³„ ì´ ì–´í…ì…˜ (ì—´ í•©ê³„)
        st.markdown("### 2. ì›ë³¸ë‹¨ì–´ë³„ ì´ ì–´í…ì…˜")
        src_total_attention = np.sum(Wd, axis=0)  # ê° ì†ŒìŠ¤ í† í°ì´ ë°›ëŠ” ì´ ì–´í…ì…˜
        fig, ax = plt.subplots(figsize=(10, 4))
        bars = ax.bar(src_tokens, src_total_attention)
        ax.set_title("ê° ì›ë³¸ ë‹¨ì–´ê°€ ë°›ëŠ” ì´ ì–´í…ì…˜", fontsize=12)
        ax.set_ylabel("ì´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜", fontsize=10)
        ax.set_xlabel("ì›ë³¸ ë‹¨ì–´", fontsize=10)
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, src_total_attention):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                   f'{value:.3f}', ha='center', va='bottom')
        plt.xticks(rotation=45)
        st.pyplot(fig)
        
        # 3. ë²ˆì—­ ë‹¨ì–´ë³„ ì–´í…ì…˜ ë¶„ì‚°
        st.markdown("### 3. ë²ˆì—­ ë‹¨ì–´ë³„ ì–´í…ì…˜ ë¶„ì‚°")
        tgt_attention_variance = np.var(Wd, axis=1)  # ê° íƒ€ê¹ƒ í† í°ì˜ ì–´í…ì…˜ ë¶„ì‚°
        fig, ax = plt.subplots(figsize=(10, 4))
        bars = ax.bar(tgt_tokens, tgt_attention_variance)
        ax.set_title("ê° ë²ˆì—­ ë‹¨ì–´ì˜ ì–´í…ì…˜ ë¶„ì‚°", fontsize=12)
        ax.set_ylabel("ë¶„ì‚°", fontsize=10)
        ax.set_xlabel("ë²ˆì—­ ë‹¨ì–´", fontsize=10)
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, tgt_attention_variance):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, 
                   f'{value:.3f}', ha='center', va='bottom')
        plt.xticks(rotation=45)
        st.pyplot(fig)
        
        # 4. ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        st.markdown("### 4. ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼")
        def calculate_entropy(attention_weights):
            """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
            # 0ì— ê°€ê¹Œìš´ ê°’ë“¤ì„ ì‘ì€ ì–‘ìˆ˜ë¡œ ëŒ€ì²´
            eps = 1e-10
            weights = np.maximum(attention_weights, eps)
            # ì •ê·œí™”
            weights = weights / np.sum(weights, axis=-1, keepdims=True)
            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: -sum(p * log(p))
            entropy = -np.sum(weights * np.log(weights), axis=-1)
            return entropy
        
        # ê° íƒ€ê¹ƒ í† í°ë³„ ì—”íŠ¸ë¡œí”¼
        tgt_entropy = calculate_entropy(Wd)
        fig, ax = plt.subplots(figsize=(10, 4))
        bars = ax.bar(tgt_tokens, tgt_entropy)
        ax.set_title("ê° ë²ˆì—­ ë‹¨ì–´ì˜ ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼", fontsize=12)
        ax.set_ylabel("ì—”íŠ¸ë¡œí”¼", fontsize=10)
        ax.set_xlabel("ë²ˆì—­ ë‹¨ì–´", fontsize=10)
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, tgt_entropy):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                   f'{value:.3f}', ha='center', va='bottom')
        plt.xticks(rotation=45)
        st.pyplot(fig)
        
        # 5. í†µê³„ ìš”ì•½
        st.markdown("### 5. ì–´í…ì…˜ í†µê³„ ìš”ì•½")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("í‰ê·  ì–´í…ì…˜", f"{np.mean(Wd):.4f}")
            st.metric("ìµœëŒ€ ì–´í…ì…˜", f"{np.max(Wd):.4f}")
            st.metric("ìµœì†Œ ì–´í…ì…˜", f"{np.min(Wd):.4f}")
        
        with col2:
            st.metric("ì–´í…ì…˜ í‘œì¤€í¸ì°¨", f"{np.std(Wd):.4f}")
            st.metric("ê°€ì¥ ì§‘ì¤‘ëœ íƒ€ê¹ƒ", tgt_tokens[np.argmin(tgt_entropy)])
            st.metric("ê°€ì¥ ë¶„ì‚°ëœ íƒ€ê¹ƒ", tgt_tokens[np.argmax(tgt_entropy)])
        
        with col3:
            st.metric("ê°€ì¥ ì£¼ëª©ë°›ëŠ” ì†ŒìŠ¤", src_tokens[np.argmax(src_total_attention)])
            st.metric("ê°€ì¥ ì£¼ëª©ë°›ëŠ” íƒ€ê¹ƒ", tgt_tokens[np.argmax(np.sum(Wd, axis=1))])
            st.metric("ì „ì²´ ì—”íŠ¸ë¡œí”¼", f"{np.mean(tgt_entropy):.4f}")
        
        # 6. ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„
        st.markdown("### 6. ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„")
        st.markdown("**ë†’ì€ ì—”íŠ¸ë¡œí”¼ (ë¶„ì‚°ëœ ì–´í…ì…˜)**: í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ê°€ ì—¬ëŸ¬ ì†ŒìŠ¤ ë‹¨ì–´ì— ê³ ë¥´ê²Œ ì£¼ëª©")
        st.markdown("**ë‚®ì€ ì—”íŠ¸ë¡œí”¼ (ì§‘ì¤‘ëœ ì–´í…ì…˜)**: í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ê°€ íŠ¹ì • ì†ŒìŠ¤ ë‹¨ì–´ì— ì§‘ì¤‘")
        st.markdown("**ë†’ì€ ë¶„ì‚°**: í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ì˜ ì–´í…ì…˜ì´ ë¶ˆê· ë“±í•˜ê²Œ ë¶„í¬")
        st.markdown("**ë‚®ì€ ë¶„ì‚°**: í•´ë‹¹ íƒ€ê¹ƒ ë‹¨ì–´ì˜ ì–´í…ì…˜ì´ ê· ë“±í•˜ê²Œ ë¶„í¬")

# --------------------------------------------------
# íƒ­ 4: ì„ë² ë”© ë¶„ì„
# --------------------------------------------------
with tabs[3]:
    st.subheader("ì„ë² ë”© ìœ ì‚¬ë„ & ë¶„í¬")
    if analyze:
        all_tokens = ["[SRC]:"+t for t in src_tokens] + ["[TGT]:"+t for t in tgt_tokens]
        X = np.vstack([src_E, tgt_E])
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)
        cos = Xn @ Xn.T
        plot_heatmap(cos, xticks=all_tokens, yticks=all_tokens, title="ì½”ì‚¬ì¸ ìœ ì‚¬ë„")

        st.markdown("### PCA 2D ë¶„í¬(ì‹œì—°ìš©)")
        Z = (X - X.mean(0, keepdims=True))
        U, S, Vt = np.linalg.svd(Z, full_matrices=False)
        Z2 = Z @ Vt.T[:, :2]
        fig, ax = plt.subplots()
        ax.scatter(Z2[:len(src_tokens), 0], Z2[:len(src_tokens), 1], label="SRC")
        ax.scatter(Z2[len(src_tokens):, 0], Z2[len(src_tokens):, 1], label="TGT")
        for i, t in enumerate(all_tokens):
            ax.annotate(t, (Z2[i,0], Z2[i,1]), fontsize=8)
        ax.set_title("ì„ë² ë”© ë¶„í¬ (PCA 2D)", fontsize=12)
        ax.set_xlabel("ì²« ë²ˆì§¸ ì£¼ì„±ë¶„", fontsize=10)
        ax.set_ylabel("ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„", fontsize=10)
        ax.legend()
        st.pyplot(fig)

# --------------------------------------------------
# íƒ­ 5: PyTorch êµ¬í˜„
# --------------------------------------------------
with tabs[4]:
    st.subheader("PyTorchë¡œ ìŠ¤ì¼€ì¼ë“œë‹·í”„ë¡œë•íŠ¸ & ë©€í‹°í—¤ë“œ")
    
    if analyze:
        # PyTorch í…ì„œë¡œ ë³€í™˜
        src_E_torch = torch.tensor(src_E, dtype=torch.float32)
        tgt_E_torch = torch.tensor(tgt_E, dtype=torch.float32)
        Wq_torch = torch.tensor(Wq, dtype=torch.float32)
        Wk_torch = torch.tensor(Wk, dtype=torch.float32)
        Wv_torch = torch.tensor(Wv, dtype=torch.float32)
        
        # PyTorchë¡œ ì–´í…ì…˜ ê³„ì‚°
        Q_torch = torch.matmul(src_E_torch, Wq_torch)
        K_torch = torch.matmul(src_E_torch, Wk_torch)
        V_torch = torch.matmul(src_E_torch, Wv_torch)
        
        # Scaled Dot-Product Attention
        d_k = Q_torch.size(-1)
        scores_torch = torch.matmul(Q_torch, K_torch.transpose(-2, -1)) / math.sqrt(d_k)
        weights_torch = torch.softmax(scores_torch, dim=-1)
        context_torch = torch.matmul(weights_torch, V_torch)
        
        st.write("PyTorch ì–´í…ì…˜ ê²°ê³¼ shape:", context_torch.shape)
        st.dataframe(np_to_df(context_torch.detach().numpy(), row_idx=src_tokens))

# --------------------------------------------------
# íƒ­ 6: AI ì±—ë´‡ (ê°„ë‹¨ ì˜ˆì‹œ)
# --------------------------------------------------
with tabs[5]:
    st.subheader("ğŸ”¬ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

    if analyze:
        # ì–´í…ì…˜ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
        Qd = linear_projection(tgt_E, Wq)
        Ks = linear_projection(src_E, Wk)
        Vs = linear_projection(src_E, Wv)
        _, Wd, _ = scaled_dot_product_attention(Qd, Ks, Vs)
        
        st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì§ˆë¬¸")
        
        if len(tgt_tokens) > 0 and len(src_tokens) > 0:
            q1 = f"'{tgt_tokens[0]}' ë‹¨ì–´ê°€ ê°€ì¥ ì£¼ëª©í•œ ì›ë³¸ ë‹¨ì–´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            if st.button(q1):
                focused_idx = np.argmax(Wd[0])
                focused_word = src_tokens[focused_idx]
                st.write(f"**ë‹µë³€:** '{focused_word}' ë‹¨ì–´ì…ë‹ˆë‹¤. (ì–´í…ì…˜ ìŠ¤ì½”ì–´: {Wd[0, focused_idx]:.3f})")

            if len(src_tokens) > 1:
                q2 = f"'{src_tokens[1]}' ë‹¨ì–´ëŠ” ì–´ë–¤ ë²ˆì—­ ë‹¨ì–´ë¡œë¶€í„° ê°€ì¥ ë§ì€ ì£¼ëª©ì„ ë°›ì•˜ë‚˜ìš”?"
                if st.button(q2):
                    attending_idx = np.argmax(Wd[:, 1])
                    attending_word = tgt_tokens[attending_idx]
                    st.write(f"**ë‹µë³€:** '{attending_word}' ë‹¨ì–´ì…ë‹ˆë‹¤. (ì–´í…ì…˜ ìŠ¤ì½”ì–´: {Wd[attending_idx, 1]:.3f})")
            
            # ì¶”ê°€ ì§ˆë¬¸ë“¤
            q3 = "ê°€ì¥ ë†’ì€ ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ê°€ì§„ ë‹¨ì–´ ìŒì€ ë¬´ì—‡ì¸ê°€ìš”?"
            if st.button(q3):
                max_idx = np.unravel_index(np.argmax(Wd), Wd.shape)
                max_score = Wd[max_idx]
                tgt_word = tgt_tokens[max_idx[0]]
                src_word = src_tokens[max_idx[1]]
                st.write(f"**ë‹µë³€:** '{tgt_word}' â†’ '{src_word}' (ì–´í…ì…˜ ìŠ¤ì½”ì–´: {max_score:.3f})")
            
            q4 = "ì–´ë–¤ ì˜ì–´ ë‹¨ì–´ê°€ ê°€ì¥ ë¶„ì‚°ëœ ì–´í…ì…˜ì„ ë³´ì´ë‚˜ìš”?"
            if st.button(q4):
                # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
                def calculate_entropy(attention_weights):
                    eps = 1e-10
                    weights = np.maximum(attention_weights, eps)
                    weights = weights / np.sum(weights, axis=-1, keepdims=True)
                    entropy = -np.sum(weights * np.log(weights), axis=-1)
                    return entropy
                
                entropies = calculate_entropy(Wd)
                most_distributed_idx = np.argmax(entropies)
                most_distributed_word = tgt_tokens[most_distributed_idx]
                st.write(f"**ë‹µë³€:** '{most_distributed_word}' ë‹¨ì–´ì…ë‹ˆë‹¤. (ì—”íŠ¸ë¡œí”¼: {entropies[most_distributed_idx]:.3f})")
        
        st.markdown("---")
        st.markdown("### ğŸ’¬ ììœ  ì§ˆë¬¸")
        
        # ì‚¬ìš©ìê°€ ì§ì ‘ ì§ˆë¬¸í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë„ ìœ ì§€
        user_input = st.text_input("ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš” (ì˜ˆ: ì–´í…ì…˜ì´ë€?):", "")
        if st.button("ì§ˆë¬¸í•˜ê¸°"):
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ëª¨ë¸ ì‚¬ìš©)
            responses = {
                "ì–´í…ì…˜": "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ íŠ¹ì • ë¶€ë¶„ì— ì§‘ì¤‘í•˜ì—¬ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
                "transformer": "TransformerëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.",
                "self-attention": "Self-attentionì€ ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ì˜ ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ì„ ì°¸ì¡°í•˜ëŠ” ì–´í…ì…˜ì…ë‹ˆë‹¤.",
                "cross-attention": "Cross-attentionì€ ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê°„ì˜ ì–´í…ì…˜ì…ë‹ˆë‹¤.",
                "ë©€í‹°í—¤ë“œ": "ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì€ ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ë™ì‹œì— íŒŒì•…í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
                "ë§ˆìŠ¤í‚¹": "ë§ˆìŠ¤í‚¹ì€ ë””ì½”ë”ì—ì„œ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•˜ì—¬ í•™ìŠµ ì‹œ ì •ë‹µì„ ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.",
                "q": "Query(ì§ˆë¬¸)ëŠ” í˜„ì¬ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºì–´ì•¼ í• ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë˜ì§€ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.",
                "k": "Key(í‚¤)ëŠ” ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì´ 'ë‚˜ ì´ëŸ° ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì–´!'ë¼ê³  ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.",
                "v": "Value(ê°’)ëŠ” ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ, Queryì™€ ê°€ì¥ ì˜ ë§ëŠ” Keyì— ì—°ê²°ëœ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
            }
            
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ì–´í…ì…˜', 'transformer', 'self-attention', 'cross-attention', 'ë©€í‹°í—¤ë“œ', 'ë§ˆìŠ¤í‚¹', 'q', 'k', 'v' ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
            for keyword, resp in responses.items():
                if keyword.lower() in user_input.lower():
                    response = resp
                    break
            
            st.write("**AI ì‘ë‹µ:**", response)
            st.info("ğŸ’¡ ì´ëŠ” êµìœ¡ìš© ë°ëª¨ì…ë‹ˆë‹¤. ì‹¤ì œ AI ì±—ë´‡ì€ ë” ì •êµí•œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    else:
        st.warning("ë¨¼ì € 'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")